# -*- coding: utf-8 -*-
"""Relatorio AVD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F0Qq0KvbBRivHuppzFzMSmhAjaGDI3UV

# Relat√≥rio de An√°lise e Visualiza√ß√£o de Dados

Neste relat√≥rio iremos analisar os dados de duas entrevista feitas por:

- Francisca Guedes, a sua irm√£ Eduarda Guedes de 25 anos. A sua entrevista vai estar nomeada como "Entrevista_dada" (Entrevista 1)
- Mariana Martins, ao seu tio Jos√© Morgado de 72 anos. A sua entrevista vai estar nomeada como "Entrevista_tio" (Entrevista 2)

### M√©tricas

* Lemas e a sua frequ√™ncia
* Entidades
* Classes Gramaticais
* Total de Palavras
  * Com stopwords
  * Sem stopwords
* 10 palavras mais frequentes
* Tamanho M√©dio de Frases
* An√°lise de Sentimentos

### Fazer upload das entrevistas no Collab
"""

# Instalar e importar o necess√°rio
!pip install -q spacy
!python -m spacy download pt_core_news_sm

import spacy
from google.colab import files

# Carregar modelo
nlp = spacy.load("pt_core_news_sm")

# Upload uma √∫nica vez
uploaded = files.upload()

# Ler e guardar o texto numa vari√°vel global
for file_name in uploaded:
    with open(file_name, 'r', encoding='utf-8') as f:
        Entrevista_dada = f.read()

print("‚úÖ Texto carregado e armazenado na vari√°vel 'Entrevista_dada'")

import spacy
from google.colab import files

# Carregar modelo
nlp = spacy.load("pt_core_news_sm")

# Upload uma √∫nica vez
uploaded = files.upload()

# Ler e guardar o texto numa vari√°vel global
for file_name in uploaded:
    with open(file_name, 'r', encoding='utf-8') as f:
        Entrevista_tio = f.read()

print("‚úÖ Texto carregado e armazenado na vari√°vel 'Entrevista_tio'")

"""## Lemas E Entidades

### Lematiza√ß√£o e a sua Frequ√™ncia

Este c√≥digo identifica as palavras -> frequ√™ncia -> lema. Depois guarda os resultados num ficheiro csv

### Entrevista 1
"""

import csv
from collections import Counter

# Processar o texto armazenado
doc = nlp(Entrevista_dada)

# Filtrar tokens: excluir stopwords, pontua√ß√£o, espa√ßos e n√£o-palavras
tokens_filtrados = [
    (token.text.lower(), token.lemma_.lower())
    for token in doc
    if not token.is_stop and not token.is_punct and token.is_alpha
]

# Criar um dicion√°rio de frequ√™ncia por palavra original (ignorando caixa)
frequencia = Counter([palavra for palavra, lema in tokens_filtrados])

# Criar um dicion√°rio para associar cada palavra ao seu lema (sem repeti√ß√µes)
lemas_por_palavra = {}
for palavra, lema in tokens_filtrados:
    if palavra not in lemas_por_palavra:
        lemas_por_palavra[palavra] = lema

# Mostrar resultados: palavra original, frequ√™ncia e lema
print("üìå Palavra | Frequ√™ncia | Lema")
print("-" * 30)
for palavra, contagem in frequencia.items():
    print(f"{palavra:<10} | {contagem:^10} | {lemas_por_palavra[palavra]}")

# Fun√ß√£o para exportar resultados para CSV
def exportar_para_csv(nome_arquivo="resultado_palavras.csv"):
    with open(nome_arquivo, mode='w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Palavra", "Frequ√™ncia", "Lema"])
        for palavra, contagem in frequencia.items():
            writer.writerow([palavra, contagem, lemas_por_palavra[palavra]])
    print(f"\n‚úÖ Resultados exportados para '{nome_arquivo}'")

# Chamada opcional para exportar
exportar_para_csv()

"""Gr√°fico

- Alguns gr√°ficos que demonstra√ß√£o os lemas mais frequentes e tamb√©m a estat√≠stica
"""

# Instalar bibliotecas necess√°rias (execute apenas uma vez)
!pip install plotly spacy matplotlib seaborn

# Importar bibliotecas
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import Counter
import spacy

# Seu c√≥digo original (adaptado)
# Assumindo que voc√™ j√° tem o nlp e Entrevista_dada definidos
# nlp = spacy.load("pt_core_news_sm")  # Descomente se necess√°rio

# Processar o texto armazenado
doc = nlp(Entrevista_dada)

# Filtrar tokens: excluir stopwords, pontua√ß√£o, espa√ßos e n√£o-palavras
tokens_filtrados = [
    (token.text.lower(), token.lemma_.lower())
    for token in doc
    if not token.is_stop and not token.is_punct and token.is_alpha
]

# Criar um dicion√°rio de frequ√™ncia por lema
frequencia_lemas = Counter([lema for palavra, lema in tokens_filtrados])

# Obter os 20 lemas mais frequentes
top_20_lemas = frequencia_lemas.most_common(20)

# Preparar dados para visualiza√ß√£o
lemas = [item[0] for item in top_20_lemas]
frequencias = [item[1] for item in top_20_lemas]

# Criar DataFrame para facilitar manipula√ß√£o
df = pd.DataFrame({
    'Lema': lemas,
    'Frequ√™ncia': frequencias,
    'Rank': range(1, 21)
})

print("üìå Top 20 Lemas Mais Frequentes:")
print("-" * 35)
for i, (lema, freq) in enumerate(top_20_lemas, 1):
    print(f"{i:2d}. {lema:<15} | {freq:>4} ocorr√™ncias")

# ========== GR√ÅFICO INTERATIVO COM PLOTLY ==========

# Definir cores vibrantes
cores = [
    '#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57',
    '#FF9FF3', '#54A0FF', '#5F27CD', '#00D2D3', '#FF9F43',
    '#FF6348', '#7BED9F', '#70A1FF', '#5352ED', '#FF4757',
    '#2ED573', '#3742FA', '#F368E0', '#FFA502', '#FF3838'
]

# 1. Gr√°fico de Barras Interativo Principal
fig1 = go.Figure()

fig1.add_trace(go.Bar(
    x=lemas,
    y=frequencias,
    marker=dict(
        color=cores[:20],
        line=dict(color='rgba(0,0,0,0.1)', width=1)
    ),
    text=frequencias,
    textposition='auto',
    textfont=dict(size=12, color='white', family='Arial Black'),
    hovertemplate='<b>%{x}</b><br>' +
                  'Frequ√™ncia: %{y}<br>' +
                  'Ranking: #%{customdata}<br>' +
                  '<extra></extra>',
    customdata=df['Rank']
))

fig1.update_layout(
    title={
        'text': 'üìä Top 20 Lemas Mais Frequentes',
        'x': 0.5,
        'xanchor': 'center',
        'font': {'size': 24, 'family': 'Arial Black', 'color': '#2C3E50'}
    },
    xaxis=dict(
        title='Lemas',
        tickangle=-45,
        tickfont=dict(size=12, family='Arial'),
        showgrid=True,
        gridcolor='rgba(128,128,128,0.2)'
    ),
    yaxis=dict(
        title='Frequ√™ncia',
        tickfont=dict(size=12, family='Arial'),
        showgrid=True,
        gridcolor='rgba(128,128,128,0.2)'
    ),
    plot_bgcolor='rgba(248,249,250,0.8)',
    paper_bgcolor='white',
    showlegend=False,
    width=1000,
    height=600,
    margin=dict(l=80, r=80, t=100, b=120)
)

fig1.show()

# 2. Gr√°fico de Pizza/Rosca Interativo
fig2 = go.Figure()

fig2.add_trace(go.Pie(
    labels=lemas,
    values=frequencias,
    hole=0.4,
    marker=dict(colors=cores[:20], line=dict(color='white', width=2)),
    textinfo='label+percent',
    textposition='auto',
    hovertemplate='<b>%{label}</b><br>' +
                  'Frequ√™ncia: %{value}<br>' +
                  'Percentual: %{percent}<br>' +
                  '<extra></extra>'
))

fig2.update_layout(
    title={
        'text': 'üç© Distribui√ß√£o dos 20 Lemas Mais Frequentes',
        'x': 0.5,
        'xanchor': 'center',
        'font': {'size': 24, 'family': 'Arial Black', 'color': '#2C3E50'}
    },
    width=800,
    height=600,
    showlegend=True,
    legend=dict(
        orientation="v",
        yanchor="middle",
        y=0.5,
        xanchor="left",
        x=1.02
    )
)

fig2.show()

# 3. Gr√°fico de Barras Horizontais
fig3 = go.Figure()

fig3.add_trace(go.Bar(
    x=frequencias,
    y=lemas,
    orientation='h',
    marker=dict(color=cores[:20]),
    text=frequencias,
    textposition='auto',
    textfont=dict(size=11, color='white', family='Arial Black')
))

fig3.update_layout(
    title={
        'text': 'üìà Ranking de Lemas (Horizontal)',
        'x': 0.5,
        'xanchor': 'center',
        'font': {'size': 24, 'family': 'Arial Black', 'color': '#2C3E50'}
    },
    xaxis_title='Frequ√™ncia',
    yaxis_title='Lemas',
    yaxis=dict(categoryorder='total ascending'),
    plot_bgcolor='rgba(248,249,250,0.8)',
    width=900,
    height=700
)

fig3.show()

# 4. Gr√°fico combinado com subplots
fig4 = make_subplots(
    rows=2, cols=2,
    subplot_titles=('Top 10 Lemas', '√öltimos 10 Lemas', 'Distribui√ß√£o Acumulada', 'Estat√≠sticas'),
    specs=[[{"type": "bar"}, {"type": "bar"}],
           [{"type": "scatter"}, {"type": "table"}]]
)

# Top 10
fig4.add_trace(
    go.Bar(x=lemas[:10], y=frequencias[:10],
           marker_color=cores[:10], name="Top 10"),
    row=1, col=1
)

# √öltimos 10
fig4.add_trace(
    go.Bar(x=lemas[10:], y=frequencias[10:],
           marker_color=cores[10:20], name="√öltimos 10"),
    row=1, col=2
)

# Distribui√ß√£o acumulada
freq_acumulada = [sum(frequencias[:i+1]) for i in range(len(frequencias))]
fig4.add_trace(
    go.Scatter(x=list(range(1, 21)), y=freq_acumulada,
               mode='lines+markers', name="Acumulado",
               line=dict(color='#FF6B6B', width=3)),
    row=2, col=1
)

# Tabela de estat√≠sticas
stats_data = [
    ['Total de Lemas', len(lemas)],
    ['Frequ√™ncia Total', sum(frequencias)],
    ['Frequ√™ncia M√©dia', round(sum(frequencias)/len(frequencias), 1)],
    ['Frequ√™ncia M√°xima', max(frequencias)],
    ['Frequ√™ncia M√≠nima', min(frequencias)],
    ['Desvio Padr√£o', round(pd.Series(frequencias).std(), 1)]
]

fig4.add_trace(
    go.Table(
        header=dict(values=['Estat√≠stica', 'Valor'],
                   fill_color='#667EEA', font_color='white'),
        cells=dict(values=list(zip(*stats_data)),
                  fill_color='#F8F9FA')
    ),
    row=2, col=2
)

fig4.update_layout(
    title_text="üìä Dashboard Completo de An√°lise de Lemas",
    title_font_size=20,
    showlegend=False,
    height=800
)

fig4.show()

# ========== GR√ÅFICO EST√ÅTICO COM MATPLOTLIB/SEABORN ==========

# Configurar estilo
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Criar figura com m√∫ltiplos subplots
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('üìä An√°lise Completa dos 20 Lemas Mais Frequentes',
             fontsize=20, fontweight='bold', y=0.98)

# 1. Gr√°fico de barras vertical
bars1 = ax1.bar(range(len(lemas)), frequencias, color=cores[:20])
ax1.set_title('Frequ√™ncia por Lema', fontsize=14, fontweight='bold')
ax1.set_xlabel('Lemas')
ax1.set_ylabel('Frequ√™ncia')
ax1.set_xticks(range(len(lemas)))
ax1.set_xticklabels(lemas, rotation=45, ha='right')

# Adicionar valores nas barras
for i, bar in enumerate(bars1):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
             f'{int(height)}', ha='center', va='bottom', fontweight='bold')

# 2. Gr√°fico de barras horizontal
bars2 = ax2.barh(range(len(lemas)), frequencias, color=cores[:20])
ax2.set_title('Ranking de Lemas', fontsize=14, fontweight='bold')
ax2.set_xlabel('Frequ√™ncia')
ax2.set_yticks(range(len(lemas)))
ax2.set_yticklabels(lemas)
ax2.invert_yaxis()

# 3. Gr√°fico de linha - distribui√ß√£o acumulada
ax3.plot(range(1, 21), freq_acumulada, marker='o', linewidth=3,
         markersize=8, color='#FF6B6B')
ax3.fill_between(range(1, 21), freq_acumulada, alpha=0.3, color='#FF6B6B')
ax3.set_title('Distribui√ß√£o Acumulada', fontsize=14, fontweight='bold')
ax3.set_xlabel('Ranking')
ax3.set_ylabel('Frequ√™ncia Acumulada')
ax3.grid(True, alpha=0.3)

# 4. Boxplot das frequ√™ncias
ax4.boxplot(frequencias, patch_artist=True,
           boxprops=dict(facecolor='#4ECDC4', alpha=0.7),
           medianprops=dict(color='#FF6B6B', linewidth=2))
ax4.set_title('Distribui√ß√£o das Frequ√™ncias', fontsize=14, fontweight='bold')
ax4.set_ylabel('Frequ√™ncia')

# Adicionar estat√≠sticas no boxplot
stats_text = f'''Estat√≠sticas:
M√©dia: {sum(frequencias)/len(frequencias):.1f}
Mediana: {sorted(frequencias)[len(frequencias)//2]:.1f}
Max: {max(frequencias)}
Min: {min(frequencias)}'''

ax4.text(1.1, max(frequencias)*0.8, stats_text,
         bbox=dict(boxstyle="round,pad=0.3", facecolor='lightblue', alpha=0.7))

plt.tight_layout()
plt.show()

# ========== RELAT√ìRIO FINAL ==========
print("\n" + "="*60)
print("üìà RELAT√ìRIO DE AN√ÅLISE DE LEMAS")
print("="*60)
print(f"üìä Total de lemas √∫nicos analisados: {len(frequencia_lemas)}")
print(f"üî¢ Frequ√™ncia total dos top 20: {sum(frequencias)}")
print(f"üìà Frequ√™ncia m√©dia dos top 20: {sum(frequencias)/len(frequencias):.1f}")
print(f"üèÜ Lema mais frequente: '{lemas[0]}' ({frequencias[0]} ocorr√™ncias)")
print(f"ü•â Lema menos frequente (top 20): '{lemas[-1]}' ({frequencias[-1]} ocorr√™ncias)")
print("="*60)

# Criar DataFrame final para exporta√ß√£o (opcional)
df_final = pd.DataFrame({
    'Ranking': range(1, 21),
    'Lema': lemas,
    'Frequ√™ncia': frequencias,
    'Percentual': [f"{(freq/sum(frequencias)*100):.1f}%" for freq in frequencias]
})

print("\nüìã Tabela Final dos Top 20 Lemas:")
print(df_final.to_string(index=False))

# Para salvar a tabela (descomente se necess√°rio)
# df_final.to_csv('top_20_lemas.csv', index=False)
# print("\nüíæ Tabela salva como 'top_20_lemas.csv'")

"""### Entrevista 2"""

import csv
from collections import Counter

# Processar o texto armazenado
doc = nlp(Entrevista_dada)

# Filtrar tokens: excluir stopwords, pontua√ß√£o, espa√ßos e n√£o-palavras
tokens_filtrados = [
    (token.text.lower(), token.lemma_.lower())
    for token in doc
    if not token.is_stop and not token.is_punct and token.is_alpha
]

# Criar um dicion√°rio de frequ√™ncia por palavra original (ignorando caixa)
frequencia = Counter([palavra for palavra, lema in tokens_filtrados])

# Criar um dicion√°rio para associar cada palavra ao seu lema (sem repeti√ß√µes)
lemas_por_palavra = {}
for palavra, lema in tokens_filtrados:
    if palavra not in lemas_por_palavra:
        lemas_por_palavra[palavra] = lema

# Mostrar resultados: palavra original, frequ√™ncia e lema
print("üìå Palavra | Frequ√™ncia | Lema")
print("-" * 30)
for palavra, contagem in frequencia.items():
    print(f"{palavra:<10} | {contagem:^10} | {lemas_por_palavra[palavra]}")

# Fun√ß√£o para exportar resultados para CSV
def exportar_para_csv(nome_arquivo="resultado_palavras.csv"):
    with open(nome_arquivo, mode='w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Palavra", "Frequ√™ncia", "Lema"])
        for palavra, contagem in frequencia.items():
            writer.writerow([palavra, contagem, lemas_por_palavra[palavra]])
    print(f"\n‚úÖ Resultados exportados para '{nome_arquivo}'")

# Chamada opcional para exportar
exportar_para_csv()

# Instalar bibliotecas necess√°rias (execute apenas uma vez)
!pip install plotly spacy matplotlib seaborn

# Importar bibliotecas
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import Counter
import spacy

# Seu c√≥digo original (adaptado)
# Assumindo que voc√™ j√° tem o nlp e Entrevista_dada definidos
# nlp = spacy.load("pt_core_news_sm")  # Descomente se necess√°rio

# Processar o texto armazenado
doc = nlp(Entrevista_tio)

# Filtrar tokens: excluir stopwords, pontua√ß√£o, espa√ßos e n√£o-palavras
tokens_filtrados = [
    (token.text.lower(), token.lemma_.lower())
    for token in doc
    if not token.is_stop and not token.is_punct and token.is_alpha
]

# Criar um dicion√°rio de frequ√™ncia por lema
frequencia_lemas = Counter([lema for palavra, lema in tokens_filtrados])

# Obter os 20 lemas mais frequentes
top_20_lemas = frequencia_lemas.most_common(20)

# Preparar dados para visualiza√ß√£o
lemas = [item[0] for item in top_20_lemas]
frequencias = [item[1] for item in top_20_lemas]

# Criar DataFrame para facilitar manipula√ß√£o
df = pd.DataFrame({
    'Lema': lemas,
    'Frequ√™ncia': frequencias,
    'Rank': range(1, 21)
})

print("üìå Top 20 Lemas Mais Frequentes:")
print("-" * 35)
for i, (lema, freq) in enumerate(top_20_lemas, 1):
    print(f"{i:2d}. {lema:<15} | {freq:>4} ocorr√™ncias")

# ========== GR√ÅFICO INTERATIVO COM PLOTLY ==========

# Definir cores vibrantes
cores = [
    '#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57',
    '#FF9FF3', '#54A0FF', '#5F27CD', '#00D2D3', '#FF9F43',
    '#FF6348', '#7BED9F', '#70A1FF', '#5352ED', '#FF4757',
    '#2ED573', '#3742FA', '#F368E0', '#FFA502', '#FF3838'
]

# 1. Gr√°fico de Barras Interativo Principal
fig1 = go.Figure()

fig1.add_trace(go.Bar(
    x=lemas,
    y=frequencias,
    marker=dict(
        color=cores[:20],
        line=dict(color='rgba(0,0,0,0.1)', width=1)
    ),
    text=frequencias,
    textposition='auto',
    textfont=dict(size=12, color='white', family='Arial Black'),
    hovertemplate='<b>%{x}</b><br>' +
                  'Frequ√™ncia: %{y}<br>' +
                  'Ranking: #%{customdata}<br>' +
                  '<extra></extra>',
    customdata=df['Rank']
))

fig1.update_layout(
    title={
        'text': 'üìä Top 20 Lemas Mais Frequentes',
        'x': 0.5,
        'xanchor': 'center',
        'font': {'size': 24, 'family': 'Arial Black', 'color': '#2C3E50'}
    },
    xaxis=dict(
        title='Lemas',
        tickangle=-45,
        tickfont=dict(size=12, family='Arial'),
        showgrid=True,
        gridcolor='rgba(128,128,128,0.2)'
    ),
    yaxis=dict(
        title='Frequ√™ncia',
        tickfont=dict(size=12, family='Arial'),
        showgrid=True,
        gridcolor='rgba(128,128,128,0.2)'
    ),
    plot_bgcolor='rgba(248,249,250,0.8)',
    paper_bgcolor='white',
    showlegend=False,
    width=1000,
    height=600,
    margin=dict(l=80, r=80, t=100, b=120)
)

fig1.show()

# 2. Gr√°fico de Pizza/Rosca Interativo
fig2 = go.Figure()

fig2.add_trace(go.Pie(
    labels=lemas,
    values=frequencias,
    hole=0.4,
    marker=dict(colors=cores[:20], line=dict(color='white', width=2)),
    textinfo='label+percent',
    textposition='auto',
    hovertemplate='<b>%{label}</b><br>' +
                  'Frequ√™ncia: %{value}<br>' +
                  'Percentual: %{percent}<br>' +
                  '<extra></extra>'
))

fig2.update_layout(
    title={
        'text': 'üç© Distribui√ß√£o dos 20 Lemas Mais Frequentes',
        'x': 0.5,
        'xanchor': 'center',
        'font': {'size': 24, 'family': 'Arial Black', 'color': '#2C3E50'}
    },
    width=800,
    height=600,
    showlegend=True,
    legend=dict(
        orientation="v",
        yanchor="middle",
        y=0.5,
        xanchor="left",
        x=1.02
    )
)

fig2.show()

# 3. Gr√°fico de Barras Horizontais
fig3 = go.Figure()

fig3.add_trace(go.Bar(
    x=frequencias,
    y=lemas,
    orientation='h',
    marker=dict(color=cores[:20]),
    text=frequencias,
    textposition='auto',
    textfont=dict(size=11, color='white', family='Arial Black')
))

fig3.update_layout(
    title={
        'text': 'üìà Ranking de Lemas (Horizontal)',
        'x': 0.5,
        'xanchor': 'center',
        'font': {'size': 24, 'family': 'Arial Black', 'color': '#2C3E50'}
    },
    xaxis_title='Frequ√™ncia',
    yaxis_title='Lemas',
    yaxis=dict(categoryorder='total ascending'),
    plot_bgcolor='rgba(248,249,250,0.8)',
    width=900,
    height=700
)

fig3.show()

# 4. Gr√°fico combinado com subplots
fig4 = make_subplots(
    rows=2, cols=2,
    subplot_titles=('Top 10 Lemas', '√öltimos 10 Lemas', 'Distribui√ß√£o Acumulada', 'Estat√≠sticas'),
    specs=[[{"type": "bar"}, {"type": "bar"}],
           [{"type": "scatter"}, {"type": "table"}]]
)

# Top 10
fig4.add_trace(
    go.Bar(x=lemas[:10], y=frequencias[:10],
           marker_color=cores[:10], name="Top 10"),
    row=1, col=1
)

# √öltimos 10
fig4.add_trace(
    go.Bar(x=lemas[10:], y=frequencias[10:],
           marker_color=cores[10:20], name="√öltimos 10"),
    row=1, col=2
)

# Distribui√ß√£o acumulada
freq_acumulada = [sum(frequencias[:i+1]) for i in range(len(frequencias))]
fig4.add_trace(
    go.Scatter(x=list(range(1, 21)), y=freq_acumulada,
               mode='lines+markers', name="Acumulado",
               line=dict(color='#FF6B6B', width=3)),
    row=2, col=1
)

# Tabela de estat√≠sticas
stats_data = [
    ['Total de Lemas', len(lemas)],
    ['Frequ√™ncia Total', sum(frequencias)],
    ['Frequ√™ncia M√©dia', round(sum(frequencias)/len(frequencias), 1)],
    ['Frequ√™ncia M√°xima', max(frequencias)],
    ['Frequ√™ncia M√≠nima', min(frequencias)],
    ['Desvio Padr√£o', round(pd.Series(frequencias).std(), 1)]
]

fig4.add_trace(
    go.Table(
        header=dict(values=['Estat√≠stica', 'Valor'],
                   fill_color='#667EEA', font_color='white'),
        cells=dict(values=list(zip(*stats_data)),
                  fill_color='#F8F9FA')
    ),
    row=2, col=2
)

fig4.update_layout(
    title_text="üìä Dashboard Completo de An√°lise de Lemas",
    title_font_size=20,
    showlegend=False,
    height=800
)

fig4.show()

# ========== GR√ÅFICO EST√ÅTICO COM MATPLOTLIB/SEABORN ==========

# Configurar estilo
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Criar figura com m√∫ltiplos subplots
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('üìä An√°lise Completa dos 20 Lemas Mais Frequentes',
             fontsize=20, fontweight='bold', y=0.98)

# 1. Gr√°fico de barras vertical
bars1 = ax1.bar(range(len(lemas)), frequencias, color=cores[:20])
ax1.set_title('Frequ√™ncia por Lema', fontsize=14, fontweight='bold')
ax1.set_xlabel('Lemas')
ax1.set_ylabel('Frequ√™ncia')
ax1.set_xticks(range(len(lemas)))
ax1.set_xticklabels(lemas, rotation=45, ha='right')

# Adicionar valores nas barras
for i, bar in enumerate(bars1):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
             f'{int(height)}', ha='center', va='bottom', fontweight='bold')

# 2. Gr√°fico de barras horizontal
bars2 = ax2.barh(range(len(lemas)), frequencias, color=cores[:20])
ax2.set_title('Ranking de Lemas', fontsize=14, fontweight='bold')
ax2.set_xlabel('Frequ√™ncia')
ax2.set_yticks(range(len(lemas)))
ax2.set_yticklabels(lemas)
ax2.invert_yaxis()

# 3. Gr√°fico de linha - distribui√ß√£o acumulada
ax3.plot(range(1, 21), freq_acumulada, marker='o', linewidth=3,
         markersize=8, color='#FF6B6B')
ax3.fill_between(range(1, 21), freq_acumulada, alpha=0.3, color='#FF6B6B')
ax3.set_title('Distribui√ß√£o Acumulada', fontsize=14, fontweight='bold')
ax3.set_xlabel('Ranking')
ax3.set_ylabel('Frequ√™ncia Acumulada')
ax3.grid(True, alpha=0.3)

# 4. Boxplot das frequ√™ncias
ax4.boxplot(frequencias, patch_artist=True,
           boxprops=dict(facecolor='#4ECDC4', alpha=0.7),
           medianprops=dict(color='#FF6B6B', linewidth=2))
ax4.set_title('Distribui√ß√£o das Frequ√™ncias', fontsize=14, fontweight='bold')
ax4.set_ylabel('Frequ√™ncia')

# Adicionar estat√≠sticas no boxplot
stats_text = f'''Estat√≠sticas:
M√©dia: {sum(frequencias)/len(frequencias):.1f}
Mediana: {sorted(frequencias)[len(frequencias)//2]:.1f}
Max: {max(frequencias)}
Min: {min(frequencias)}'''

ax4.text(1.1, max(frequencias)*0.8, stats_text,
         bbox=dict(boxstyle="round,pad=0.3", facecolor='lightblue', alpha=0.7))

plt.tight_layout()
plt.show()

# ========== RELAT√ìRIO FINAL ==========
print("\n" + "="*60)
print("üìà RELAT√ìRIO DE AN√ÅLISE DE LEMAS")
print("="*60)
print(f"üìä Total de lemas √∫nicos analisados: {len(frequencia_lemas)}")
print(f"üî¢ Frequ√™ncia total dos top 20: {sum(frequencias)}")
print(f"üìà Frequ√™ncia m√©dia dos top 20: {sum(frequencias)/len(frequencias):.1f}")
print(f"üèÜ Lema mais frequente: '{lemas[0]}' ({frequencias[0]} ocorr√™ncias)")
print(f"ü•â Lema menos frequente (top 20): '{lemas[-1]}' ({frequencias[-1]} ocorr√™ncias)")
print("="*60)

# Criar DataFrame final para exporta√ß√£o (opcional)
df_final = pd.DataFrame({
    'Ranking': range(1, 21),
    'Lema': lemas,
    'Frequ√™ncia': frequencias,
    'Percentual': [f"{(freq/sum(frequencias)*100):.1f}%" for freq in frequencias]
})

print("\nüìã Tabela Final dos Top 20 Lemas:")
print(df_final.to_string(index=False))

# Para salvar a tabela (descomente se necess√°rio)
# df_final.to_csv('top_20_lemas.csv', index=False)
# print("\nüíæ Tabela salva como 'top_20_lemas.csv'")

"""#### Compara√ß√£o em Gr√°fico

Dois gr√°ficos que comparam as duas entrevistas:
  - o primeiro cont√©m as palavras frequentes de ambos
  - o segundo connt√©m as palavras frequentes que tem em comum
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Dados das entrevistas
entrevista_1 = {
    'achar': 11, 'pessoa': 10, 'vida': 10, 'aprender': 8, 'jovem': 6, 'importante': 6,
    'fam√≠lia': 4, 'gostar': 4, 'objetivo': 4, 'tirar': 4, 'carta': 4, 'estudar': 4,
    'hoje': 4, 'trabalho': 4, 'escola': 3, 'bocadinho': 3, 'gostava': 3, 'marcante': 3,
    'acha': 3, 'ter': 3
}

entrevista_2 = {
    'pessoa': 25, 'ter': 23, 'vida': 20, 'achar': 19, 'gostar': 18, 'importante': 18,
    'trabalho': 16, 'viver': 16, 'haver': 15, 'gente': 14, 'ir': 12, 'pensar': 12,
    'muito': 12, 'falar': 12, 'coisa': 10, 'trabalhar': 10, 'hoje': 10, 'jovem': 9,
    'estudar': 9, 'escola': 8
}

# Obter todas as palavras √∫nicas das duas entrevistas
todas_palavras = set(entrevista_1.keys()) | set(entrevista_2.keys())

# Criar DataFrame para facilitar a visualiza√ß√£o
df_data = []
for palavra in todas_palavras:
    freq_1 = entrevista_1.get(palavra, 0)
    freq_2 = entrevista_2.get(palavra, 0)
    df_data.append({
        'palavra': palavra,
        'entrevista_1': freq_1,
        'entrevista_2': freq_2,
        'total': freq_1 + freq_2
    })

df = pd.DataFrame(df_data)
# Ordenar por frequ√™ncia total (decrescente)
df = df.sort_values('total', ascending=False)

# Configurar o gr√°fico
plt.figure(figsize=(15, 10))

# Definir posi√ß√µes das barras
x = np.arange(len(df))
width = 0.35

# Criar as barras
bars1 = plt.bar(x - width/2, df['entrevista_1'], width,
                label='Entrevista 1', alpha=0.8, color='skyblue')
bars2 = plt.bar(x + width/2, df['entrevista_2'], width,
                label='Entrevista 2', alpha=0.8, color='lightcoral')

# Personalizar o gr√°fico
plt.xlabel('Palavras', fontsize=12, fontweight='bold')
plt.ylabel('Frequ√™ncia', fontsize=12, fontweight='bold')
plt.title('Compara√ß√£o de Frequ√™ncia de Palavras - Entrevistas 1 e 2',
          fontsize=14, fontweight='bold', pad=20)
plt.xticks(x, df['palavra'], rotation=45, ha='right')
plt.legend()
plt.grid(axis='y', alpha=0.3)

# Adicionar valores nas barras
for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):
    height1 = bar1.get_height()
    height2 = bar2.get_height()

    if height1 > 0:
        plt.text(bar1.get_x() + bar1.get_width()/2., height1 + 0.1,
                f'{int(height1)}', ha='center', va='bottom', fontsize=8)

    if height2 > 0:
        plt.text(bar2.get_x() + bar2.get_width()/2., height2 + 0.1,
                f'{int(height2)}', ha='center', va='bottom', fontsize=8)

plt.tight_layout()
plt.show()

# Mostrar estat√≠sticas b√°sicas
print("=== ESTAT√çSTICAS DAS ENTREVISTAS ===")
print(f"Entrevista 1: {len(entrevista_1)} palavras √∫nicas")
print(f"Entrevista 2: {len(entrevista_2)} palavras √∫nicas")
print(f"Total de palavras √∫nicas: {len(todas_palavras)}")
print(f"Palavras em comum: {len(set(entrevista_1.keys()) & set(entrevista_2.keys()))}")

print("\n=== TOP 10 PALAVRAS MAIS FREQUENTES ===")
print(df.head(10)[['palavra', 'entrevista_1', 'entrevista_2', 'total']].to_string(index=False))

# Gr√°fico adicional: apenas palavras em comum
palavras_comuns = set(entrevista_1.keys()) & set(entrevista_2.keys())
if palavras_comuns:
    df_comuns = df[df['palavra'].isin(palavras_comuns)].copy()

    plt.figure(figsize=(12, 8))
    x_comuns = np.arange(len(df_comuns))

    bars1_comuns = plt.bar(x_comuns - width/2, df_comuns['entrevista_1'], width,
                          label='Entrevista 1', alpha=0.8, color='darkblue')
    bars2_comuns = plt.bar(x_comuns + width/2, df_comuns['entrevista_2'], width,
                          label='Entrevista 2', alpha=0.8, color='darkred')

    plt.xlabel('Palavras Comuns', fontsize=12, fontweight='bold')
    plt.ylabel('Frequ√™ncia', fontsize=12, fontweight='bold')
    plt.title('Compara√ß√£o - Apenas Palavras Presentes em Ambas as Entrevistas',
              fontsize=14, fontweight='bold', pad=20)
    plt.xticks(x_comuns, df_comuns['palavra'], rotation=45, ha='right')
    plt.legend()
    plt.grid(axis='y', alpha=0.3)

    # Adicionar valores nas barras
    for i, (bar1, bar2) in enumerate(zip(bars1_comuns, bars2_comuns)):
        height1 = bar1.get_height()
        height2 = bar2.get_height()

        plt.text(bar1.get_x() + bar1.get_width()/2., height1 + 0.1,
                f'{int(height1)}', ha='center', va='bottom', fontsize=9)
        plt.text(bar2.get_x() + bar2.get_width()/2., height2 + 0.1,
                f'{int(height2)}', ha='center', va='bottom', fontsize=9)

    plt.tight_layout()
    plt.show()

"""### Entidades

Entidades nomeadas s√£o palavras ou express√µes que se referem a coisas "conhecidas", como:

- PER (Pessoa): Jos√© Saramago
- LOC (Local/Localiza√ß√£o): Portugal
- DATE (Data): 1992
- ORG (Organiza√ß√£o): ONU

##### 1¬™ Tentativa

- N√£o consegue identificar as entidades corretamente
- Algumas est√£o certas e outras erradas

### Entrevista 1
"""

import spacy
import pandas as pd
import csv

# Carregar modelo de linguagem em portugu√™s
nlp = spacy.load("pt_core_news_sm")

# Processar o texto
doc = nlp(Entrevista_dada)

# Extrair entidades
dados_entidades = []
for ent in doc.ents:
    dados_entidades.append({
        "Entidade": ent.text,
        "Tipo": ent.label_,
        "In√≠cio": ent.start_char,
        "Fim": ent.end_char
    })

# Criar DataFrame
df_entidades = pd.DataFrame(dados_entidades)

# Mostrar como tabela (no Jupyter, por exemplo)
print(df_entidades)

# Exportar para CSV
arquivo_csv = "entidades_extraidas.csv"
df_entidades.to_csv(arquivo_csv, index=False, encoding="utf-8")
print(f"\n‚úÖ Arquivo CSV salvo como '{arquivo_csv}'")

"""### Entrevista 2"""

import spacy
import pandas as pd
import csv

# Carregar modelo de linguagem em portugu√™s
nlp = spacy.load("pt_core_news_sm")

# Processar o texto
doc = nlp(Entrevista_tio)

# Extrair entidades
dados_entidades = []
for ent in doc.ents:
    dados_entidades.append({
        "Entidade": ent.text,
        "Tipo": ent.label_,
        "In√≠cio": ent.start_char,
        "Fim": ent.end_char
    })

# Criar DataFrame
df_entidades = pd.DataFrame(dados_entidades)

# Mostrar como tabela (no Jupyter, por exemplo)
print(df_entidades)

# Exportar para CSV
arquivo_csv = "entidades_extraidas.csv"
df_entidades.to_csv(arquivo_csv, index=False, encoding="utf-8")
print(f"\n‚úÖ Arquivo CSV salvo como '{arquivo_csv}'")

"""##### 2¬™ Tentativa

- Muito parecido a primeira tentativa
- Cont√©m erros tamb√©m
- Cont√©m a descri√ß√£o de cada entidade

### Entrevista 1
"""

# Importa√ß√µes
import spacy
import pandas as pd
from spacy import displacy
from IPython.display import display

# Processa o texto
doc = nlp(Entrevista_dada)

# Extrai todas as entidades do texto
entidades = []
for ent in doc.ents:
    descricao = spacy.explain(ent.label_)
    entidades.append({
        "Texto": ent.text,
        "Tipo (label_)": ent.label_,
        "Descri√ß√£o": descricao if descricao else "‚Äì",
        "In√≠cio": ent.start_char,
        "Fim": ent.end_char
    })

# Cria DataFrame com resultados
df_entidades = pd.DataFrame(entidades)

# Mostra a tabela
print("üè∑Ô∏è Entidades nomeadas encontradas:")
display(df_entidades)

# Visualiza√ß√£o colorida (opcional)
displacy.render(doc, style="ent", jupyter=True)

"""### Entrevista 2"""

# Importa√ß√µes
import spacy
import pandas as pd
from spacy import displacy
from IPython.display import display

# Processa o texto
doc = nlp(Entrevista_tio)

# Extrai todas as entidades do texto
entidades = []
for ent in doc.ents:
    descricao = spacy.explain(ent.label_)
    entidades.append({
        "Texto": ent.text,
        "Tipo (label_)": ent.label_,
        "Descri√ß√£o": descricao if descricao else "‚Äì",
        "In√≠cio": ent.start_char,
        "Fim": ent.end_char
    })

# Cria DataFrame com resultados
df_entidades = pd.DataFrame(entidades)

# Mostra a tabela
print("üè∑Ô∏è Entidades nomeadas encontradas:")
display(df_entidades)

# Visualiza√ß√£o colorida (opcional)
displacy.render(doc, style="ent", jupyter=True)

"""##### 3¬™ Tentaiva
- Apenas encontra 15 entidades e tamb√©m est√£o erradas

"""

# Instalar e importar depend√™ncias
!python -m spacy download pt_core_news_sm

import spacy
from collections import Counter
import matplotlib.pyplot as plt
import pandas as pd

# Carregar modelo spaCy
nlp = spacy.load("pt_core_news_sm")

# Palavras que queremos excluir por erro frequente
palavras_excluir = {
    "Tarefa", "Dif√≠cil", "Campo", "Humildade", "Mem√≥rias", "tempestade", "D√°vamo-nos", "Podiam"
}

# Tipos de entidade permitidos
tipos_validos = {"PER", "LOC", "ORG", "DATE", "GPE"}

# ‚úÖ Carregar o texto diretamente do arquivo .txt
with open("Entrevista_dada.txt", "r", encoding="utf-8") as file:
    texto = file.read()

# ‚úÖ Processar com spaCy
doc = nlp(texto)

# ‚úÖ Extrair entidades filtradas
entidades_filtradas = [
    (ent.text.strip(), ent.label_)
    for ent in doc.ents
    if ent.label_ in tipos_validos
    and ent.text.strip() not in palavras_excluir
    and ent.text.strip()[0].isupper()
]

# ‚úÖ Contar frequ√™ncia
frequencias = Counter(entidades_filtradas)

# ‚úÖ Converter para DataFrame
df_entidades = pd.DataFrame(
    [(texto, tipo, freq) for (texto, tipo), freq in frequencias.items()],
    columns=["Entidade", "Tipo", "Frequ√™ncia"]
).sort_values(by="Frequ√™ncia", ascending=False)

# ‚úÖ Guardar como CSV
df_entidades.to_csv("entidades_frequencia.csv", index=False)

# ‚úÖ Gr√°fico Top 10 Entidades
top_10 = df_entidades.head(10)
plt.figure(figsize=(10, 6))
plt.bar(top_10["Entidade"], top_10["Frequ√™ncia"], color="orange")
plt.title("Top 10 Entidades mais Frequentes")
plt.xlabel("Entidade")
plt.ylabel("Frequ√™ncia")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("entidades_top10.png")
plt.show()

"""### Alternativa √†s Entidades -> Classe Gramatical

Classes Gramaticais usadas:

1. **Nome comum (NOUN)**: Palavra que designa coisas, pessoas, animais, lugares ou ideias de forma geral (n√£o espec√≠fica).

2.  **Nome pr√≥prio (PROPN)**: Palavra que nomeia algo espec√≠fico, como um nome de pessoa, pa√≠s, empresa, etc. Come√ßa com letra mai√∫scula.

3. **Verbo (VERB)**: Palavra que indica a√ß√£o, estado ou ocorr√™ncia

4. **Adjetivo (ADJ)**: Palavra que qualifica ou caracteriza um nome

5.  **Adv√©rbio (ADV)**: Palavra que modifica um verbo, um adjetivo ou outro adv√©rbio, indicando modo, tempo, intensidade, etc

6. **Preposi√ß√£o (ADP)**: Palavra que liga palavras/frases, indicando rela√ß√µes (de lugar, tempo, causa...)


- O c√≥digo separa as palavras e atribu√≠-lhe uma classe gramaticas. Guarda tamb√©m os resultados em ficheiro csv

### Entrevista 1
"""

# Importa√ß√µes
import spacy
import pandas as pd
from IPython.display import display

# Carregar modelo (certifique-se de que 'pt_core_news_sm' est√° instalado)
nlp = spacy.load("pt_core_news_sm")

# Processa o texto
doc = nlp(Entrevista_dada)

# Categorias de interesse
categorias_desejadas = {
    "NOUN": "Nome comum",
    "PROPN": "Nome pr√≥prio",
    "VERB": "Verbo",
    "ADJ": "Adjetivo",
    "ADV": "Adv√©rbio",
    "ADP": "Preposi√ß√£o"
}

# Extrair e classificar os tokens
tokens_filtrados = [
    {
        "Texto": token.text,
        "Classe Gramatical": categorias_desejadas[token.pos_],
        "Lemma": token.lemma_,
        "Posi√ß√£o (in√≠cio)": token.idx
    }
    for token in doc
    if token.pos_ in categorias_desejadas
]

# Criar DataFrame
df_tokens = pd.DataFrame(tokens_filtrados)

# Adicionar coluna de frequ√™ncia de ocorr√™ncia da palavra
df_tokens["Frequ√™ncia"] = df_tokens.groupby("Texto")["Texto"].transform("count")

# Mostrar tabela com os resultados
print("üîç Palavras identificadas por categoria gramatical:")
display(df_tokens)

# Contagem por classe gramatical
contagem = df_tokens["Classe Gramatical"].value_counts().reset_index()
contagem.columns = ["Classe Gramatical", "Quantidade"]

# Mostrar contagem
print("\nüìä Contagem por categoria gramatical:")
display(contagem)

# üíæ Salvar os resultados em arquivos CSV
df_tokens.to_csv("palavras_com_classe_gramatical.csv", index=False, encoding="utf-8")

"""### Entrevista 2"""

# Importa√ß√µes
import spacy
import pandas as pd
from IPython.display import display

# Carregar modelo (certifique-se de que 'pt_core_news_sm' est√° instalado)
nlp = spacy.load("pt_core_news_sm")

# Processa o texto
doc = nlp(Entrevista_tio)

# Categorias de interesse
categorias_desejadas = {
    "NOUN": "Nome comum",
    "PROPN": "Nome pr√≥prio",
    "VERB": "Verbo",
    "ADJ": "Adjetivo",
    "ADV": "Adv√©rbio",
    "ADP": "Preposi√ß√£o"
}

# Extrair e classificar os tokens
tokens_filtrados = [
    {
        "Texto": token.text,
        "Classe Gramatical": categorias_desejadas[token.pos_],
        "Lemma": token.lemma_,
        "Posi√ß√£o (in√≠cio)": token.idx
    }
    for token in doc
    if token.pos_ in categorias_desejadas
]

# Criar DataFrame
df_tokens = pd.DataFrame(tokens_filtrados)

# Adicionar coluna de frequ√™ncia de ocorr√™ncia da palavra
df_tokens["Frequ√™ncia"] = df_tokens.groupby("Texto")["Texto"].transform("count")

# Mostrar tabela com os resultados
print("üîç Palavras identificadas por categoria gramatical:")
display(df_tokens)

# Contagem por classe gramatical
contagem = df_tokens["Classe Gramatical"].value_counts().reset_index()
contagem.columns = ["Classe Gramatical", "Quantidade"]

# Mostrar contagem
print("\nüìä Contagem por categoria gramatical:")
display(contagem)

# üíæ Salvar os resultados em arquivos CSV
df_tokens.to_csv("palavras_com_classe_gramatical.csv", index=False, encoding="utf-8")

"""#### Compara√ß√£o em Gr√°fico"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Dados das classes gramaticais
entrevista_1 = {
    'Nome': 169,
    'Verbo': 148,
    'Preposi√ß√£o': 103,
    'Adv√©rbio': 93,
    'Adjetivo': 58,
    'Nome Pr√≥prio': 14
}

entrevista_2 = {
    'Verbo': 476,
    'Nome': 424,
    'Adv√©rbio': 357,
    'Preposi√ß√£o': 240,
    'Adjetivo': 139,
    'Nome Pr√≥prio': 28
}

# Criar DataFrame para facilitar a visualiza√ß√£o
df_data = []
for classe in entrevista_1.keys():
    freq_1 = entrevista_1[classe]
    freq_2 = entrevista_2[classe]
    df_data.append({
        'classe_gramatical': classe,
        'entrevista_1': freq_1,
        'entrevista_2': freq_2,
        'total': freq_1 + freq_2,
        'diferenca': freq_2 - freq_1
    })

df = pd.DataFrame(df_data)
# Ordenar por frequ√™ncia da entrevista 2 (decrescente)
df = df.sort_values('entrevista_2', ascending=False)

# Configurar o gr√°fico principal
plt.figure(figsize=(14, 10))

# Definir posi√ß√µes das barras
x = np.arange(len(df))
width = 0.35

# Criar as barras
bars1 = plt.bar(x - width/2, df['entrevista_1'], width,
                label='Entrevista 1', alpha=0.8, color='steelblue')
bars2 = plt.bar(x + width/2, df['entrevista_2'], width,
                label='Entrevista 2', alpha=0.8, color='orange')

# Personalizar o gr√°fico
plt.xlabel('Classes Gramaticais', fontsize=12, fontweight='bold')
plt.ylabel('Frequ√™ncia', fontsize=12, fontweight='bold')
plt.title('An√°lise Comparativa de Classes Gramaticais - Entrevistas 1 e 2',
          fontsize=14, fontweight='bold', pad=20)
plt.xticks(x, df['classe_gramatical'], rotation=45, ha='right')
plt.legend()
plt.grid(axis='y', alpha=0.3)

# Adicionar valores nas barras
for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):
    height1 = bar1.get_height()
    height2 = bar2.get_height()

    plt.text(bar1.get_x() + bar1.get_width()/2., height1 + 5,
            f'{int(height1)}', ha='center', va='bottom', fontsize=10, fontweight='bold')
    plt.text(bar2.get_x() + bar2.get_width()/2., height2 + 5,
            f'{int(height2)}', ha='center', va='bottom', fontsize=10, fontweight='bold')

plt.tight_layout()
plt.show()

# Gr√°fico de percentuais
plt.figure(figsize=(12, 8))

# Calcular percentuais
total_1 = sum(entrevista_1.values())
total_2 = sum(entrevista_2.values())

df['perc_entrevista_1'] = (df['entrevista_1'] / total_1) * 100
df['perc_entrevista_2'] = (df['entrevista_2'] / total_2) * 100

# Criar gr√°fico de percentuais
bars1_perc = plt.bar(x - width/2, df['perc_entrevista_1'], width,
                     label='Entrevista 1 (%)', alpha=0.8, color='lightgreen')
bars2_perc = plt.bar(x + width/2, df['perc_entrevista_2'], width,
                     label='Entrevista 2 (%)', alpha=0.8, color='salmon')

plt.xlabel('Classes Gramaticais', fontsize=12, fontweight='bold')
plt.ylabel('Percentual (%)', fontsize=12, fontweight='bold')
plt.title('Distribui√ß√£o Percentual das Classes Gramaticais',
          fontsize=14, fontweight='bold', pad=20)
plt.xticks(x, df['classe_gramatical'], rotation=45, ha='right')
plt.legend()
plt.grid(axis='y', alpha=0.3)

# Adicionar valores percentuais nas barras
for i, (bar1, bar2) in enumerate(zip(bars1_perc, bars2_perc)):
    height1 = bar1.get_height()
    height2 = bar2.get_height()

    plt.text(bar1.get_x() + bar1.get_width()/2., height1 + 0.2,
            f'{height1:.1f}%', ha='center', va='bottom', fontsize=9)
    plt.text(bar2.get_x() + bar2.get_width()/2., height2 + 0.2,
            f'{height2:.1f}%', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()

# Gr√°fico de pizza para cada entrevista
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))

# Pizza Entrevista 1
colors1 = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc', '#c2c2f0']
wedges1, texts1, autotexts1 = ax1.pie(df['entrevista_1'], labels=df['classe_gramatical'],
                                       autopct='%1.1f%%', startangle=90, colors=colors1)
ax1.set_title('Distribui√ß√£o - Entrevista 1\n(Total: {} palavras)'.format(total_1),
              fontsize=12, fontweight='bold')

# Pizza Entrevista 2
colors2 = ['#ffb3ba', '#bae1ff', '#baffc9', '#ffffba', '#ffdfba', '#e0e0e0']
wedges2, texts2, autotexts2 = ax2.pie(df['entrevista_2'], labels=df['classe_gramatical'],
                                       autopct='%1.1f%%', startangle=90, colors=colors2)
ax2.set_title('Distribui√ß√£o - Entrevista 2\n(Total: {} palavras)'.format(total_2),
              fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()

# Mostrar estat√≠sticas detalhadas
print("=== AN√ÅLISE ESTAT√çSTICA DAS CLASSES GRAMATICAIS ===")
print(f"Total de palavras - Entrevista 1: {total_1}")
print(f"Total de palavras - Entrevista 2: {total_2}")
print(f"Diferen√ßa total: {total_2 - total_1} palavras ({((total_2/total_1)-1)*100:.1f}% de aumento)")

print("\n=== FREQU√äNCIAS E PERCENTUAIS POR CLASSE ===")
resultado = df[['classe_gramatical', 'entrevista_1', 'entrevista_2', 'diferenca',
                'perc_entrevista_1', 'perc_entrevista_2']].copy()
resultado.columns = ['Classe Gramatical', 'Entrevista 1', 'Entrevista 2',
                    'Diferen√ßa', '% Entrevista 1', '% Entrevista 2']
print(resultado.to_string(index=False, float_format='%.1f'))

print("\n=== RANKING POR FREQU√äNCIA ===")
print("Entrevista 1:")
for i, row in df.iterrows():
    print(f"{i+1}¬∫ {row['classe_gramatical']}: {row['entrevista_1']} ({row['perc_entrevista_1']:.1f}%)")

print("\nEntrevista 2:")
for i, row in df.iterrows():
    print(f"{i+1}¬∫ {row['classe_gramatical']}: {row['entrevista_2']} ({row['perc_entrevista_2']:.1f}%)")

print("\n=== OBSERVA√á√ïES ===")
max_crescimento = df.loc[df['diferenca'].idxmax()]
print(f"Maior crescimento absoluto: {max_crescimento['classe_gramatical']} (+{max_crescimento['diferenca']} palavras)")

df['crescimento_perc'] = ((df['entrevista_2'] / df['entrevista_1']) - 1) * 100
max_crescimento_perc = df.loc[df['crescimento_perc'].idxmax()]
print(f"Maior crescimento percentual: {max_crescimento_perc['classe_gramatical']} (+{max_crescimento_perc['crescimento_perc']:.1f}%)")

classe_dominante_1 = df.loc[df['entrevista_1'].idxmax()]['classe_gramatical']
classe_dominante_2 = df.loc[df['entrevista_2'].idxmax()]['classe_gramatical']
print(f"Classe mais frequente - Entrevista 1: {classe_dominante_1}")
print(f"Classe mais frequente - Entrevista 2: {classe_dominante_2}")

"""## Total de Palavras

### Entrevista 1

- Com stopwords
"""

# Processar o texto
doc = nlp(Entrevista_dada)

# Contar todas as palavras (tokens alfab√©ticos)
palavras = [token.text for token in doc if token.is_alpha]
total_palavras = len(palavras)

print(f"üî¢ Total de palavras (com stopwords): {total_palavras}")

"""- Sem stopwords"""

# Contar palavras excluindo stopwords
palavras_sem_stopwords = [
    token.text for token in doc if token.is_alpha and not token.is_stop
]
total_sem_stopwords = len(palavras_sem_stopwords)

print(f"üî¢ Total de palavras (sem stopwords): {total_sem_stopwords}")

"""#### Gr√°fico"""

import matplotlib.pyplot as plt
import numpy as np

# Dados
categorias = ['Com Stopwords', 'Sem Stopwords']
valores = [957, 337]
cores = ['#e67e22', '#f1c40f']  # Laranja e Amarelo

# Configurar o gr√°fico
plt.figure(figsize=(10, 6))
bars = plt.bar(categorias, valores, color=cores, alpha=0.8, edgecolor='black', linewidth=1.5)

# Personalizar o gr√°fico
plt.title('üìä An√°lise de Contagem de Palavras', fontsize=16, fontweight='bold', pad=20)
plt.ylabel('N√∫mero de Palavras', fontsize=12, fontweight='bold')
plt.xlabel('Tipo de Contagem', fontsize=12, fontweight='bold')

# Adicionar valores nas barras
for bar, valor in zip(bars, valores):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 10,
             f'{valor}', ha='center', va='bottom', fontsize=14, fontweight='bold')

# Configurar grade e estilo
plt.grid(axis='y', alpha=0.3, linestyle='--')
plt.ylim(0, max(valores) * 1.1)

# Melhorar apar√™ncia
plt.tight_layout()
plt.xticks(fontsize=11, fontweight='bold')
plt.yticks(fontsize=10)

# Adicionar informa√ß√µes extras
total_stopwords = valores[0] - valores[1]
percentual_stopwords = (total_stopwords / valores[0]) * 100

plt.figtext(0.5, 0.02, f'Stopwords removidas: {total_stopwords} ({percentual_stopwords:.1f}% do texto original)',
           ha='center', fontsize=10, style='italic')

# Mostrar o gr√°fico
plt.show()

# Exibir estat√≠sticas
print("="*50)
print("üìà ESTAT√çSTICAS DA AN√ÅLISE DE TEXTO")
print("="*50)
print(f"Total de palavras (com stopwords): {valores[0]}")
print(f"Total de palavras (sem stopwords): {valores[1]}")
print(f"Stopwords removidas: {total_stopwords}")
print(f"Percentual de stopwords: {percentual_stopwords:.1f}%")
print(f"Redu√ß√£o do texto: {((total_stopwords/valores[0])*100):.1f}%")
print("="*50)

"""### Entrevista 2"""

# Processar o texto
doc = nlp(Entrevista_tio)

# Contar todas as palavras (tokens alfab√©ticos)
palavras = [token.text for token in doc if token.is_alpha]
total_palavras = len(palavras)

print(f"üî¢ Total de palavras (com stopwords): {total_palavras}")

# Contar palavras excluindo stopwords
palavras_sem_stopwords = [
    token.text for token in doc if token.is_alpha and not token.is_stop
]
total_sem_stopwords = len(palavras_sem_stopwords)

print(f"üî¢ Total de palavras (sem stopwords): {total_sem_stopwords}")

import matplotlib.pyplot as plt
import numpy as np

# Dados
categorias = ['Com Stopwords', 'Sem Stopwords']
valores = [2728, 925]
cores = ['#3B82F6', '#9333EA']  # Roxo e Azul

# Configurar o gr√°fico
plt.figure(figsize=(10, 6))
bars = plt.bar(categorias, valores, color=cores, alpha=0.8, edgecolor='black', linewidth=1.5)

# Personalizar o gr√°fico
plt.title('üìä An√°lise de Contagem de Palavras', fontsize=16, fontweight='bold', pad=20)
plt.ylabel('N√∫mero de Palavras', fontsize=12, fontweight='bold')
plt.xlabel('Tipo de Contagem', fontsize=12, fontweight='bold')

# Adicionar valores nas barras
for bar, valor in zip(bars, valores):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 10,
             f'{valor}', ha='center', va='bottom', fontsize=14, fontweight='bold')

# Configurar grade e estilo
plt.grid(axis='y', alpha=0.3, linestyle='--')
plt.ylim(0, max(valores) * 1.1)

# Melhorar apar√™ncia
plt.tight_layout()
plt.xticks(fontsize=11, fontweight='bold')
plt.yticks(fontsize=10)

# Adicionar informa√ß√µes extras
total_stopwords = valores[0] - valores[1]
percentual_stopwords = (total_stopwords / valores[0]) * 100

plt.figtext(0.5, 0.02, f'Stopwords removidas: {total_stopwords} ({percentual_stopwords:.1f}% do texto original)',
           ha='center', fontsize=10, style='italic')

# Mostrar o gr√°fico
plt.show()

# Exibir estat√≠sticas
print("="*50)
print("üìà ESTAT√çSTICAS DA AN√ÅLISE DE TEXTO")
print("="*50)
print(f"Total de palavras (com stopwords): {valores[0]}")
print(f"Total de palavras (sem stopwords): {valores[1]}")
print(f"Stopwords removidas: {total_stopwords}")
print(f"Percentual de stopwords: {percentual_stopwords:.1f}%")
print(f"Redu√ß√£o do texto: {((total_stopwords/valores[0])*100):.1f}%")
print("="*50)

"""#### Compara√ß√£o em gr√°fico"""

import matplotlib.pyplot as plt
import numpy as np

# Dados das entrevistas
entrevista_1 = {'Com stopwords': 957, 'Sem stopwords': 337}
entrevista_2 = {'Com stopwords': 2728, 'Sem stopwords': 925}

# Preparar dados para o gr√°fico
categorias = ['Com stopwords', 'Sem stopwords']
valores_ent1 = [entrevista_1['Com stopwords'], entrevista_1['Sem stopwords']]
valores_ent2 = [entrevista_2['Com stopwords'], entrevista_2['Sem stopwords']]

# Configurar o gr√°fico
plt.figure(figsize=(12, 8))

# Definir posi√ß√µes das barras
x = np.arange(len(categorias))
width = 0.35

# Criar as barras
bars1 = plt.bar(x - width/2, valores_ent1, width,
                label='Entrevista 1', alpha=0.8, color='#4CAF50')
bars2 = plt.bar(x + width/2, valores_ent2, width,
                label='Entrevista 2', alpha=0.8, color='#FF7043')

# Personalizar o gr√°fico
plt.xlabel('Tipo de An√°lise', fontsize=14, fontweight='bold')
plt.ylabel('N√∫mero de Palavras', fontsize=14, fontweight='bold')
plt.title('Compara√ß√£o: An√°lise com e sem Stopwords\nEntrevistas 1 e 2',
          fontsize=16, fontweight='bold', pad=20)
plt.xticks(x, categorias, fontsize=12)
plt.legend(fontsize=12)
plt.grid(axis='y', alpha=0.3)

# Adicionar valores nas barras
for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):
    height1 = bar1.get_height()
    height2 = bar2.get_height()

    plt.text(bar1.get_x() + bar1.get_width()/2., height1 + 20,
            f'{int(height1)}', ha='center', va='bottom',
            fontsize=12, fontweight='bold')
    plt.text(bar2.get_x() + bar2.get_width()/2., height2 + 20,
            f'{int(height2)}', ha='center', va='bottom',
            fontsize=12, fontweight='bold')

# Calcular e mostrar percentuais de redu√ß√£o
reducao_ent1 = ((entrevista_1['Com stopwords'] - entrevista_1['Sem stopwords']) / entrevista_1['Com stopwords']) * 100
reducao_ent2 = ((entrevista_2['Com stopwords'] - entrevista_2['Sem stopwords']) / entrevista_2['Com stopwords']) * 100

plt.text(0, max(valores_ent1 + valores_ent2) * 0.85,
         f'Redu√ß√£o: {reducao_ent1:.1f}%',
         ha='center', va='center', fontsize=11,
         bbox=dict(boxstyle="round,pad=0.3", facecolor='lightgreen', alpha=0.7))

plt.text(1, max(valores_ent1 + valores_ent2) * 0.85,
         f'Redu√ß√£o: {reducao_ent2:.1f}%',
         ha='center', va='center', fontsize=11,
         bbox=dict(boxstyle="round,pad=0.3", facecolor='lightsalmon', alpha=0.7))

plt.tight_layout()
plt.show()

# Mostrar estat√≠sticas
print("=== AN√ÅLISE DE STOPWORDS ===")
print(f"Entrevista 1:")
print(f"  Com stopwords: {entrevista_1['Com stopwords']} palavras")
print(f"  Sem stopwords: {entrevista_1['Sem stopwords']} palavras")
print(f"  Redu√ß√£o: {reducao_ent1:.1f}% ({entrevista_1['Com stopwords'] - entrevista_1['Sem stopwords']} palavras removidas)")

print(f"\nEntrevista 2:")
print(f"  Com stopwords: {entrevista_2['Com stopwords']} palavras")
print(f"  Sem stopwords: {entrevista_2['Sem stopwords']} palavras")
print(f"  Redu√ß√£o: {reducao_ent2:.1f}% ({entrevista_2['Com stopwords'] - entrevista_2['Sem stopwords']} palavras removidas)")

print(f"\nCompara√ß√£o entre entrevistas:")
print(f"  Propor√ß√£o total (com stopwords): Entrevista 2 √© {entrevista_2['Com stopwords']/entrevista_1['Com stopwords']:.1f}x maior")
print(f"  Propor√ß√£o total (sem stopwords): Entrevista 2 √© {entrevista_2['Sem stopwords']/entrevista_1['Sem stopwords']:.1f}x maior")

"""## Top 10 palavras mais frequentes

### Entrevista 1
"""

import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
import pandas as pd

# Seu c√≥digo original
from collections import Counter

doc = nlp(Entrevista_dada)

palavras = [
    token.lemma_.lower()
    for token in doc
    if not token.is_stop and not token.is_punct and token.is_alpha
]

frequencia = Counter(palavras)

print("üîù Top 10 palavras mais frequentes:")
for palavra, freq in frequencia.most_common(10):
    print(f"{palavra}: {freq}")

# üìä CRIANDO O GR√ÅFICO INTERATIVO
# Preparar dados para visualiza√ß√£o
top_10 = frequencia.most_common(10)
palavras_top = [palavra for palavra, freq in top_10]
frequencias_top = [freq for palavra, freq in top_10]

# Criar DataFrame
df = pd.DataFrame({
    'palavra': palavras_top,
    'frequencia': frequencias_top,
    'rank': range(1, len(palavras_top) + 1)
})

# Reverter ordem para mostrar a palavra mais frequente no topo
df = df[::-1].reset_index(drop=True)

# üé® Gr√°fico de barras horizontal interativo
fig = go.Figure()

# Adicionar barras com gradiente de cores
colors = px.colors.sequential.Viridis_r[:len(df)]

fig.add_trace(go.Bar(
    y=df['palavra'],
    x=df['frequencia'],
    orientation='h',
    marker=dict(
        color=df['frequencia'],
        colorscale='Viridis',
        colorbar=dict(title="Frequ√™ncia"),
        line=dict(color='rgba(58, 71, 80, 0.6)', width=1)
    ),
    text=df['frequencia'],
    textposition='outside',
    hovertemplate='<b>%{y}</b><br>' +
                  'Frequ√™ncia: %{x}<br>' +
                  'Posi√ß√£o: %{customdata}¬∫<extra></extra>',
    customdata=df['rank']
))

# Personalizar layout
fig.update_layout(
    title={
        'text': 'üìä Top 10 Palavras Mais Frequentes',
        'x': 0.5,
        'xanchor': 'center',
        'font': {'size': 20, 'color': '#2E4057'}
    },
    xaxis_title='Frequ√™ncia',
    yaxis_title='Palavras',
    font=dict(size=12),
    height=600,
    width=900,
    plot_bgcolor='white',
    paper_bgcolor='white',
    margin=dict(l=120, r=100, t=80, b=50),
    hoverlabel=dict(
        bgcolor="white",
        font_size=14,
        font_family="Arial"
    )
)

# Estilizar eixos
fig.update_xaxes(
    showgrid=True,
    gridcolor='lightgray',
    gridwidth=1,
    showline=True,
    linecolor='gray'
)

fig.update_yaxes(
    showgrid=False,
    showline=True,
    linecolor='gray'
)

# Mostrar gr√°fico
fig.show()

# üìä ESTAT√çSTICAS RESUMIDAS
print("\n" + "="*50)
print("üìà ESTAT√çSTICAS GERAIS")
print("="*50)

total_palavras_unicas = len(frequencia)
total_ocorrencias = sum(frequencia.values())
palavra_mais_freq = frequencia.most_common(1)[0]
palavra_menos_freq = frequencia.most_common()[-1]

print(f"üî¢ Total de palavras √∫nicas: {total_palavras_unicas}")
print(f"üìä Total de ocorr√™ncias: {total_ocorrencias}")
print(f"üëë Palavra mais frequente: '{palavra_mais_freq[0]}' ({palavra_mais_freq[1]}x)")
print(f"üîª Palavra menos frequente: '{palavra_menos_freq[0]}' ({palavra_menos_freq[1]}x)")
print(f"üìä M√©dia de frequ√™ncia: {total_ocorrencias/total_palavras_unicas:.2f}")

# üìã Lista formatada para f√°cil visualiza√ß√£o
print(f"\n{'='*50}")
print("üìã TABELA FORMATADA - TOP 10")
print(f"{'='*50}")
print(f"{'Pos':<4} {'Palavra':<15} {'Freq':<6} {'%':<8}")
print(f"{'-'*35}")

for i, (palavra, freq) in enumerate(top_10, 1):
    percentual = (freq / total_ocorrencias) * 100
    print(f"{i:<4} {palavra:<15} {freq:<6} {percentual:.1f}%")

"""### Entrevista 2"""

import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
import pandas as pd

# Seu c√≥digo original
from collections import Counter

doc = nlp(Entrevista_tio)

palavras = [
    token.lemma_.lower()
    for token in doc
    if not token.is_stop and not token.is_punct and token.is_alpha
]

frequencia = Counter(palavras)

print("üîù Top 10 palavras mais frequentes:")
for palavra, freq in frequencia.most_common(10):
    print(f"{palavra}: {freq}")

# üìä CRIANDO O GR√ÅFICO INTERATIVO
# Preparar dados para visualiza√ß√£o
top_10 = frequencia.most_common(10)
palavras_top = [palavra for palavra, freq in top_10]
frequencias_top = [freq for palavra, freq in top_10]

# Criar DataFrame
df = pd.DataFrame({
    'palavra': palavras_top,
    'frequencia': frequencias_top,
    'rank': range(1, len(palavras_top) + 1)
})

# Reverter ordem para mostrar a palavra mais frequente no topo
df = df[::-1].reset_index(drop=True)

# üé® Gr√°fico de barras horizontal interativo
fig = go.Figure()

# Adicionar barras com gradiente de cores
colors = px.colors.sequential.Viridis_r[:len(df)]

fig.add_trace(go.Bar(
    y=df['palavra'],
    x=df['frequencia'],
    orientation='h',
    marker=dict(
        color=df['frequencia'],
        colorscale='Viridis',
        colorbar=dict(title="Frequ√™ncia"),
        line=dict(color='rgba(58, 71, 80, 0.6)', width=1)
    ),
    text=df['frequencia'],
    textposition='outside',
    hovertemplate='<b>%{y}</b><br>' +
                  'Frequ√™ncia: %{x}<br>' +
                  'Posi√ß√£o: %{customdata}¬∫<extra></extra>',
    customdata=df['rank']
))

# Personalizar layout
fig.update_layout(
    title={
        'text': 'üìä Top 10 Palavras Mais Frequentes',
        'x': 0.5,
        'xanchor': 'center',
        'font': {'size': 20, 'color': '#2E4057'}
    },
    xaxis_title='Frequ√™ncia',
    yaxis_title='Palavras',
    font=dict(size=12),
    height=600,
    width=900,
    plot_bgcolor='white',
    paper_bgcolor='white',
    margin=dict(l=120, r=100, t=80, b=50),
    hoverlabel=dict(
        bgcolor="white",
        font_size=14,
        font_family="Arial"
    )
)

# Estilizar eixos
fig.update_xaxes(
    showgrid=True,
    gridcolor='lightgray',
    gridwidth=1,
    showline=True,
    linecolor='gray'
)

fig.update_yaxes(
    showgrid=False,
    showline=True,
    linecolor='gray'
)

# Mostrar gr√°fico
fig.show()

# üìä ESTAT√çSTICAS RESUMIDAS
print("\n" + "="*50)
print("üìà ESTAT√çSTICAS GERAIS")
print("="*50)

total_palavras_unicas = len(frequencia)
total_ocorrencias = sum(frequencia.values())
palavra_mais_freq = frequencia.most_common(1)[0]
palavra_menos_freq = frequencia.most_common()[-1]

print(f"üî¢ Total de palavras √∫nicas: {total_palavras_unicas}")
print(f"üìä Total de ocorr√™ncias: {total_ocorrencias}")
print(f"üëë Palavra mais frequente: '{palavra_mais_freq[0]}' ({palavra_mais_freq[1]}x)")
print(f"üîª Palavra menos frequente: '{palavra_menos_freq[0]}' ({palavra_menos_freq[1]}x)")
print(f"üìä M√©dia de frequ√™ncia: {total_ocorrencias/total_palavras_unicas:.2f}")

# üìã Lista formatada para f√°cil visualiza√ß√£o
print(f"\n{'='*50}")
print("üìã TABELA FORMATADA - TOP 10")
print(f"{'='*50}")
print(f"{'Pos':<4} {'Palavra':<15} {'Freq':<6} {'%':<8}")
print(f"{'-'*35}")

for i, (palavra, freq) in enumerate(top_10, 1):
    percentual = (freq / total_ocorrencias) * 100
    print(f"{i:<4} {palavra:<15} {freq:<6} {percentual:.1f}%")

"""## Tamanho M√©dio de Frases

### Entrevista 1
"""

# Processar o texto
doc = nlp(Entrevista_dada)

# Obter todas as frases e contar as palavras de cada uma
tamanhos = [
    len([token for token in sent if token.is_alpha])
    for sent in doc.sents
]

# Calcular m√©dia
if tamanhos:
    media = sum(tamanhos) / len(tamanhos)
    print(f"üß† Tamanho m√©dio das frases: {media:.2f} palavras")
else:
    print("‚ö†Ô∏è Nenhuma frase encontrada.")

"""### Entrevista 2"""

# Processar o texto
doc = nlp(Entrevista_tio)

# Obter todas as frases e contar as palavras de cada uma
tamanhos = [
    len([token for token in sent if token.is_alpha])
    for sent in doc.sents
]

# Calcular m√©dia
if tamanhos:
    media = sum(tamanhos) / len(tamanhos)
    print(f"üß† Tamanho m√©dio das frases: {media:.2f} palavras")
else:
    print("‚ö†Ô∏è Nenhuma frase encontrada.")

"""#### Compara√ß√£o em gr√°fico"""

import matplotlib.pyplot as plt
import numpy as np

# Dados
entrevistas = ['Entrevista 1', 'Entrevista 2']
valores = [20.36, 8.17]
cores = ['#3498db', '#e74c3c']  # Azul e Vermelho

# Configurar o gr√°fico
plt.figure(figsize=(10, 6))
bars = plt.barh(entrevistas, valores, color=cores, alpha=0.8, edgecolor='black', linewidth=1.5)

# Personalizar o gr√°fico
plt.title('üìä Compara√ß√£o entre Entrevistas', fontsize=16, fontweight='bold', pad=20)
plt.xlabel('Valores', fontsize=12, fontweight='bold')
plt.ylabel('Entrevistas', fontsize=12, fontweight='bold')

# Definir escala do X at√© 30
plt.xlim(0, 30)

# Adicionar valores nas barras
for bar, valor in zip(bars, valores):
    width = bar.get_width()
    plt.text(width + 0.5, bar.get_y() + bar.get_height()/2.,
             f'{valor}', ha='left', va='center', fontsize=14, fontweight='bold')

# Configurar grade e estilo
plt.grid(axis='x', alpha=0.3, linestyle='--')

# Melhorar apar√™ncia
plt.tight_layout()
plt.yticks(fontsize=11, fontweight='bold')
plt.xticks(fontsize=10)

# Adicionar linha de refer√™ncia (opcional)
plt.axvline(x=15, color='green', linestyle=':', alpha=0.5, label='Refer√™ncia (15)')
plt.legend(loc='lower right')

# Mostrar o gr√°fico
plt.show()

# Exibir estat√≠sticas
print("="*50)
print("üìà ESTAT√çSTICAS DAS ENTREVISTAS")
print("="*50)
print(f"Entrevista 1: {valores[0]}")
print(f"Entrevista 2: {valores[1]}")
print(f"Diferen√ßa: {valores[0] - valores[1]:.2f}")
print(f"Raz√£o (E1/E2): {valores[0]/valores[1]:.2f}x")
print(f"M√©dia: {np.mean(valores):.2f}")
print("="*50)

"""## An√°lise de Sentimentos

Foram feitas v√°rias tentativas de an√°lise de sentimentos.

##### 1¬™ Tentativa

Aqui usamos o modelo BERT e o TextBlob, no entanto tivemos de dividir as perguntas por v√°rios blocos de c√≥digo, ou seja em cada bloco tem o mesmo c√≥digo mas o n√∫mero das perguntas v√£o mudando.
  - Primeiro bloco: pergunta 1 a 5
  - Segundo bloco: pergunta 6 a 8 (TextBlob)
  - Terceiro Bloco: 9 a 12
  - Quarto bloco: 13 a 15
  - Quinto Bloco: 16 a 19

A raz√£o pelo qual optamos por fazer assim, foi porque o modelo BERT tem um limite de tokens e ao tentar analisar as perguntas todas de uma vez n√£o suportava e s√≥ analisa at√© a pergunta 6.

### Entrevista 1

Perguntas 1 a 5
"""

from transformers import pipeline
import spacy
import re
import math

# Carrega modelos
nlp = spacy.load("pt_core_news_sm")
sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")


# Processa com SpaCy
doc = nlp(Entrevista_dada)
texto_limpo = doc.text

# Extrair blocos numerados (pergunta + resposta)
blocos = re.findall(r'\d+\.\s(.*?)(?=\n\d+\.|\Z)', texto_limpo, re.DOTALL)

# Fun√ß√£o para dividir texto em partes com at√© N tokens
def dividir_em_blocos(texto, n=512):
    palavras = texto.split()
    blocos = []
    for i in range(0, len(palavras), n):
        blocos.append(" ".join(palavras[i:i+n]))
    return blocos

# Analisar apenas as perguntas de 1 a 5
for i, bloco in enumerate(blocos[:5], 1):
    linhas = bloco.strip().split('\n')
    pergunta = linhas[0].strip()
    resposta = " ".join(linhas[1:]).strip()

    if resposta:
        blocos_resposta = dividir_em_blocos(resposta, n=512)
        pontuacoes = []
        for parte in blocos_resposta:
            resultado = sentiment_pipeline(parte)[0]
            pontuacoes.append(int(resultado["label"][0]))  # pega n√∫mero da estrela

        media = sum(pontuacoes) / len(pontuacoes)
        print(f"üîπ Pergunta {i}: {pergunta}")
        print(f"üü¢ Resposta (com {len(blocos_resposta)} partes): {resposta}")
        print(f"üìä Sentimento m√©dio: {round(media, 2)} estrelas\n")

"""Perguntas 6 a 8"""

from textblob import TextBlob
import spacy
import re

# Carrega o modelo do spaCy
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o de an√°lise de sentimento com TextBlob
def analisar_sentimento_textblob(texto):
    blob = TextBlob(texto)
    return blob.sentiment.polarity  # de -1 a 1

# Converte polaridade em estrelas (1 a 5)
def polaridade_para_estrelas(polaridade):
    if polaridade <= -0.5:
        return 1
    elif polaridade <= 0:
        return 2
    elif polaridade <= 0.3:
        return 3
    elif polaridade <= 0.6:
        return 4
    else:
        return 5

# Processa o texto
doc = nlp(Entrevista_dada)
texto_limpo = doc.text

# Extrair blocos numerados
blocos = re.findall(r'\d+\.\s(.*?)(?=\n\d+\.|\Z)', texto_limpo, re.DOTALL)

# Analisa perguntas 6 a 8 (√≠ndices 5 a 7)
for i in range(5, 8):
    if i < len(blocos):
        bloco = blocos[i]
        linhas = bloco.strip().split('\n')
        pergunta = linhas[0].strip()
        resposta = " ".join(linhas[1:]).strip()

        if resposta:
            polaridade = analisar_sentimento_textblob(resposta)
            estrelas = polaridade_para_estrelas(polaridade)
            print(f"üîπ Pergunta {i+1}: {pergunta}")
            print(f"üü¢ Resposta: {resposta}")
            print(f"üìä Polaridade: {round(polaridade, 2)}")
            print(f"‚≠ê Estrelas: {estrelas}\n")
        else:
            print(f"‚ö†Ô∏è Pergunta {i+1} sem resposta.\n")

"""Perguntas 9 a 12"""

from transformers import pipeline
import spacy
import re

# Carrega modelos
nlp = spacy.load("pt_core_news_sm")
sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# Processa com SpaCy
doc = nlp(Entrevista_dada)
texto_limpo = doc.text

# Extrair blocos numerados (pergunta + resposta)
blocos = re.findall(r'\d+\.\s(.*?)(?=\n\d+\.|\Z)', texto_limpo, re.DOTALL)

# Fun√ß√£o para dividir texto em partes com at√© N tokens (palavras, neste caso)
def dividir_em_blocos(texto, n=512):
    palavras = texto.split()
    blocos = []
    for i in range(0, len(palavras), n):
        blocos.append(" ".join(palavras[i:i+n]))
    return blocos

# Analisar perguntas 9 a 12 (√≠ndices 8 a 11)
for i in range(8, 12):
    if i >= len(blocos):
        print(f"‚ö†Ô∏è Pergunta {i+1} n√£o encontrada.")
        continue

    bloco = blocos[i]
    linhas = bloco.strip().split('\n')
    pergunta = linhas[0].strip()
    resposta = " ".join(linhas[1:]).strip()

    if resposta:
        blocos_resposta = dividir_em_blocos(resposta, n=512)
        pontuacoes = []
        for parte in blocos_resposta:
            resultado = sentiment_pipeline(parte)[0]
            pontuacoes.append(int(resultado["label"][0]))  # n√∫mero da estrela

        media = sum(pontuacoes) / len(pontuacoes)
        print(f"üîπ Pergunta {i+1}: {pergunta}")
        print(f"üü¢ Resposta (com {len(blocos_resposta)} partes): {resposta}")
        print(f"üìä Sentimento m√©dio: {round(media, 2)} estrelas\n")
    else:
        print(f"‚ùå Pergunta {i+1} n√£o possui resposta.\n")

"""Perguntas 13 a 15"""

from transformers import pipeline
import spacy
import re

# Carrega modelos
nlp = spacy.load("pt_core_news_sm")
sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# Processa com SpaCy
doc = nlp(Entrevista_dada)
texto_limpo = doc.text

# Extrair blocos numerados (pergunta + resposta)
blocos = re.findall(r'\d+\.\s(.*?)(?=\n\d+\.|\Z)', texto_limpo, re.DOTALL)

# Fun√ß√£o para dividir texto em partes com at√© N tokens (palavras)
def dividir_em_blocos(texto, n=512):
    palavras = texto.split()
    blocos = []
    for i in range(0, len(palavras), n):
        blocos.append(" ".join(palavras[i:i+n]))
    return blocos

# Analisar perguntas 13 a 15 (√≠ndices 12 a 14)
for i in range(12, 15):
    if i >= len(blocos):
        print(f"‚ö†Ô∏è Pergunta {i+1} n√£o encontrada.")
        continue

    bloco = blocos[i]
    linhas = bloco.strip().split('\n')
    pergunta = linhas[0].strip()
    resposta = " ".join(linhas[1:]).strip()

    if resposta:
        blocos_resposta = dividir_em_blocos(resposta, n=512)
        pontuacoes = []
        for parte in blocos_resposta:
            resultado = sentiment_pipeline(parte)[0]
            pontuacoes.append(int(resultado["label"][0]))  # n√∫mero da estrela

        media = sum(pontuacoes) / len(pontuacoes)
        print(f"üîπ Pergunta {i+1}: {pergunta}")
        print(f"üü¢ Resposta (com {len(blocos_resposta)} partes): {resposta}")
        print(f"üìä Sentimento m√©dio: {round(media, 2)} estrelas\n")
    else:
        print(f"‚ùå Pergunta {i+1} n√£o possui resposta.\n")

"""Perguntas 16 a 20"""

from transformers import pipeline
import spacy
import re

# Carrega modelos
nlp = spacy.load("pt_core_news_sm")
sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# Processa com SpaCy
doc = nlp(Entrevista_dada)
texto_limpo = doc.text

# Extrair blocos numerados (pergunta + resposta)
blocos = re.findall(r'\d+\.\s(.*?)(?=\n\d+\.|\Z)', texto_limpo, re.DOTALL)

# Fun√ß√£o para dividir texto em partes com at√© N tokens (palavras, neste caso)
def dividir_em_blocos(texto, n=512):
    palavras = texto.split()
    blocos = []
    for i in range(0, len(palavras), n):
        blocos.append(" ".join(palavras[i:i+n]))
    return blocos

# Analisar perguntas 16 a 20 (√≠ndices 15 a 19)
for i in range(15, 20):
    if i >= len(blocos):
        print(f"‚ö†Ô∏è Pergunta {i+1} n√£o encontrada.")
        continue

    bloco = blocos[i]
    linhas = bloco.strip().split('\n')
    pergunta = linhas[0].strip()
    resposta = " ".join(linhas[1:]).strip()

    if resposta:
        blocos_resposta = dividir_em_blocos(resposta, n=512)
        pontuacoes = []
        for parte in blocos_resposta:
            resultado = sentiment_pipeline(parte)[0]
            pontuacoes.append(int(resultado["label"][0]))  # n√∫mero da estrela

        media = sum(pontuacoes) / len(pontuacoes)
        print(f"üîπ Pergunta {i+1}: {pergunta}")
        print(f"üü¢ Resposta (com {len(blocos_resposta)} partes): {resposta}")
        print(f"üìä Sentimento m√©dio: {round(media, 2)} estrelas\n")
    else:
        print(f"‚ùå Pergunta {i+1} n√£o possui resposta.\n")

"""### Entrevista 2

Perguntas de 1 a 5
"""

from transformers import pipeline
import spacy
import re
import math

# Carrega modelos
nlp = spacy.load("pt_core_news_sm")
sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")


# Processa com SpaCy
doc = nlp(Entrevista_tio)
texto_limpo = doc.text

# Extrair blocos numerados (pergunta + resposta)
blocos = re.findall(r'\d+\.\s(.*?)(?=\n\d+\.|\Z)', texto_limpo, re.DOTALL)

# Fun√ß√£o para dividir texto em partes com at√© N tokens
def dividir_em_blocos(texto, n=512):
    palavras = texto.split()
    blocos = []
    for i in range(0, len(palavras), n):
        blocos.append(" ".join(palavras[i:i+n]))
    return blocos

# Analisar apenas as perguntas de 1 a 5
for i, bloco in enumerate(blocos[:5], 1):
    linhas = bloco.strip().split('\n')
    pergunta = linhas[0].strip()
    resposta = " ".join(linhas[1:]).strip()

    if resposta:
        blocos_resposta = dividir_em_blocos(resposta, n=512)
        pontuacoes = []
        for parte in blocos_resposta:
            resultado = sentiment_pipeline(parte)[0]
            pontuacoes.append(int(resultado["label"][0]))  # pega n√∫mero da estrela

        media = sum(pontuacoes) / len(pontuacoes)
        print(f"üîπ Pergunta {i}: {pergunta}")
        print(f"üü¢ Resposta (com {len(blocos_resposta)} partes): {resposta}")
        print(f"üìä Sentimento m√©dio: {round(media, 2)} estrelas\n")

"""Perguntas 6 a 8"""

from textblob import TextBlob
import spacy
import re

# Carrega o modelo do spaCy
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o de an√°lise de sentimento com TextBlob
def analisar_sentimento_textblob(texto):
    blob = TextBlob(texto)
    return blob.sentiment.polarity  # de -1 a 1

# Converte polaridade em estrelas (1 a 5)
def polaridade_para_estrelas(polaridade):
    if polaridade <= -0.5:
        return 1
    elif polaridade <= 0:
        return 2
    elif polaridade <= 0.3:
        return 3
    elif polaridade <= 0.6:
        return 4
    else:
        return 5

# Processa o texto
doc = nlp(Entrevista_tio)
texto_limpo = doc.text

# Extrair blocos numerados
blocos = re.findall(r'\d+\.\s(.*?)(?=\n\d+\.|\Z)', texto_limpo, re.DOTALL)

# Analisa perguntas 6 a 8 (√≠ndices 5 a 7)
for i in range(5, 8):
    if i < len(blocos):
        bloco = blocos[i]
        linhas = bloco.strip().split('\n')
        pergunta = linhas[0].strip()
        resposta = " ".join(linhas[1:]).strip()

        if resposta:
            polaridade = analisar_sentimento_textblob(resposta)
            estrelas = polaridade_para_estrelas(polaridade)
            print(f"üîπ Pergunta {i+1}: {pergunta}")
            print(f"üü¢ Resposta: {resposta}")
            print(f"üìä Polaridade: {round(polaridade, 2)}")
            print(f"‚≠ê Estrelas: {estrelas}\n")
        else:
            print(f"‚ö†Ô∏è Pergunta {i+1} sem resposta.\n")

"""Perguntas 9 a 12"""

from transformers import pipeline
import spacy
import re

# Carrega modelos
nlp = spacy.load("pt_core_news_sm")
sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# Processa com SpaCy
doc = nlp(Entrevista_tio)
texto_limpo = doc.text

# Extrair blocos numerados (pergunta + resposta)
blocos = re.findall(r'\d+\.\s(.*?)(?=\n\d+\.|\Z)', texto_limpo, re.DOTALL)

# Fun√ß√£o para dividir texto em partes com at√© N tokens (palavras, neste caso)
def dividir_em_blocos(texto, n=512):
    palavras = texto.split()
    blocos = []
    for i in range(0, len(palavras), n):
        blocos.append(" ".join(palavras[i:i+n]))
    return blocos

# Analisar perguntas 9 a 12 (√≠ndices 8 a 11)
for i in range(8, 12):
    if i >= len(blocos):
        print(f"‚ö†Ô∏è Pergunta {i+1} n√£o encontrada.")
        continue

    bloco = blocos[i]
    linhas = bloco.strip().split('\n')
    pergunta = linhas[0].strip()
    resposta = " ".join(linhas[1:]).strip()

    if resposta:
        blocos_resposta = dividir_em_blocos(resposta, n=512)
        pontuacoes = []
        for parte in blocos_resposta:
            resultado = sentiment_pipeline(parte)[0]
            pontuacoes.append(int(resultado["label"][0]))  # n√∫mero da estrela

        media = sum(pontuacoes) / len(pontuacoes)
        print(f"üîπ Pergunta {i+1}: {pergunta}")
        print(f"üü¢ Resposta (com {len(blocos_resposta)} partes): {resposta}")
        print(f"üìä Sentimento m√©dio: {round(media, 2)} estrelas\n")
    else:
        print(f"‚ùå Pergunta {i+1} n√£o possui resposta.\n")

"""Perguntas 13 a 15"""

from transformers import pipeline
import spacy
import re

# Carrega modelos
nlp = spacy.load("pt_core_news_sm")
sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# Processa com SpaCy
doc = nlp(Entrevista_tio)
texto_limpo = doc.text

# Extrair blocos numerados (pergunta + resposta)
blocos = re.findall(r'\d+\.\s(.*?)(?=\n\d+\.|\Z)', texto_limpo, re.DOTALL)

# Fun√ß√£o para dividir texto em partes com at√© N tokens (palavras)
def dividir_em_blocos(texto, n=512):
    palavras = texto.split()
    blocos = []
    for i in range(0, len(palavras), n):
        blocos.append(" ".join(palavras[i:i+n]))
    return blocos

# Analisar perguntas 13 a 15 (√≠ndices 12 a 14)
for i in range(12, 15):
    if i >= len(blocos):
        print(f"‚ö†Ô∏è Pergunta {i+1} n√£o encontrada.")
        continue

    bloco = blocos[i]
    linhas = bloco.strip().split('\n')
    pergunta = linhas[0].strip()
    resposta = " ".join(linhas[1:]).strip()

    if resposta:
        blocos_resposta = dividir_em_blocos(resposta, n=512)
        pontuacoes = []
        for parte in blocos_resposta:
            resultado = sentiment_pipeline(parte)[0]
            pontuacoes.append(int(resultado["label"][0]))  # n√∫mero da estrela

        media = sum(pontuacoes) / len(pontuacoes)
        print(f"üîπ Pergunta {i+1}: {pergunta}")
        print(f"üü¢ Resposta (com {len(blocos_resposta)} partes): {resposta}")
        print(f"üìä Sentimento m√©dio: {round(media, 2)} estrelas\n")
    else:
        print(f"‚ùå Pergunta {i+1} n√£o possui resposta.\n")

"""Perguntas 15 a 19"""

from transformers import pipeline
import spacy
import re

# Carrega modelos
nlp = spacy.load("pt_core_news_sm")
sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# Processa com SpaCy
doc = nlp(Entrevista_tio)
texto_limpo = doc.text

# Extrair blocos numerados (pergunta + resposta)
blocos = re.findall(r'\d+\.\s(.*?)(?=\n\d+\.|\Z)', texto_limpo, re.DOTALL)

# Fun√ß√£o para dividir texto em partes com at√© N tokens (palavras, neste caso)
def dividir_em_blocos(texto, n=512):
    palavras = texto.split()
    blocos = []
    for i in range(0, len(palavras), n):
        blocos.append(" ".join(palavras[i:i+n]))
    return blocos

# Analisar perguntas 16 a 20 (√≠ndices 15 a 19)
for i in range(15, 20):
    if i >= len(blocos):
        print(f"‚ö†Ô∏è Pergunta {i+1} n√£o encontrada.")
        continue

    bloco = blocos[i]
    linhas = bloco.strip().split('\n')
    pergunta = linhas[0].strip()
    resposta = " ".join(linhas[1:]).strip()

    if resposta:
        blocos_resposta = dividir_em_blocos(resposta, n=512)
        pontuacoes = []
        for parte in blocos_resposta:
            resultado = sentiment_pipeline(parte)[0]
            pontuacoes.append(int(resultado["label"][0]))  # n√∫mero da estrela

        media = sum(pontuacoes) / len(pontuacoes)
        print(f"üîπ Pergunta {i+1}: {pergunta}")
        print(f"üü¢ Resposta (com {len(blocos_resposta)} partes): {resposta}")
        print(f"üìä Sentimento m√©dio: {round(media, 2)} estrelas\n")
    else:
        print(f"‚ùå Pergunta {i+1} n√£o possui resposta.\n")

"""##### 2¬™ Tentativa"""

!pip install transformers torch --quiet

"""### Entrevista 1"""

import spacy
from transformers import pipeline

# Carregar pipeline de an√°lise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# Processar o texto com SpaCy
doc = nlp(Entrevista_dada)

# Separar por senten√ßas (ou frases) com SpaCy e analisar sentimento com Transformers
for sent in doc.sents:
    texto = sent.text.strip()
    if texto:
        resultado = sentiment_pipeline(texto)[0]
        print(f"Fala: {texto}")
        print(f"Sentimento: {resultado['label']} (Confian√ßa: {resultado['score']:.2f})\n")

"""### Entrevista 2"""

import spacy
from transformers import pipeline

# Carregar pipeline de an√°lise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# Processar o texto com SpaCy
doc = nlp(Entrevista_tio)

# Separar por senten√ßas (ou frases) com SpaCy e analisar sentimento com Transformers
for sent in doc.sents:
    texto = sent.text.strip()
    if texto:
        resultado = sentiment_pipeline(texto)[0]
        print(f"Fala: {texto}")
        print(f"Sentimento: {resultado['label']} (Confian√ßa: {resultado['score']:.2f})\n")

"""##### 3¬™ Tentativa

### Entrevista 1
"""

# Passo 2: Importar bibliotecas
from textblob import TextBlob
from google.colab import files

# Passo 3: Fazer upload do arquivo .txt
uploaded = files.upload()

# Passo 4: Ler o conte√∫do do arquivo
for filename in uploaded.keys():
    with open(filename, 'r', encoding='utf-8') as file:
        texto = file.read()

# Passo 5: An√°lise de sentimento
blob = TextBlob(texto)

# Polarity: -1 (negativo) a 1 (positivo)
# Subjectivity: 0 (objetivo) a 1 (subjetivo)
print("Texto analisado:")
print(texto[:300] + "..." if len(texto) > 300 else texto)
print("\nAn√°lise de Sentimento:")
print(f"Polaridade: {blob.sentiment.polarity}")
print(f"Subjetividade: {blob.sentiment.subjectivity}")

# Classifica√ß√£o simples
if blob.sentiment.polarity > 0:
    print("Sentimento: Positivo üòä")
elif blob.sentiment.polarity < 0:
    print("Sentimento: Negativo üò†")
else:
    print("Sentimento: Neutro üòê")

"""#### Gr√°fico

"""

# C√≥digo para Google Colab - Visualiza√ß√£o de An√°lise de Sentimento

# Importar bibliotecas necess√°rias
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from matplotlib.patches import Wedge
import warnings
warnings.filterwarnings('ignore')

# Configurar estilo dos gr√°ficos
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Dados da an√°lise de sentimento
dados_sentimento = {
    'polaridade': 0.0,
    'subjetividade': 0.1,
    'sentimento': 'Neutro',
    'emoji': 'üòê'
}

# Criar figura com subplots
fig = plt.figure(figsize=(16, 12))
fig.suptitle('üìä An√°lise de Sentimento - Dashboard', fontsize=20, fontweight='bold', y=0.95)

# 1. Gr√°fico de Polaridade (Pizza)
ax1 = plt.subplot(2, 3, 1)
polaridade = dados_sentimento['polaridade']

# Determinar categoria da polaridade
if polaridade < -0.1:
    valores_polaridade = [abs(polaridade), 0, 0]
    categoria = 'Negativo'
elif polaridade > 0.1:
    valores_polaridade = [0, 0, polaridade]
    categoria = 'Positivo'
else:
    valores_polaridade = [0, 1, 0]
    categoria = 'Neutro'

labels_polaridade = ['Negativo', 'Neutro', 'Positivo']
cores_polaridade = ['#ff6b6b', '#4ecdc4', '#45b7d1']
explode = (0.05, 0.05, 0.05)

plt.pie(valores_polaridade, labels=labels_polaridade, colors=cores_polaridade,
        explode=explode, autopct='%1.1f%%', startangle=90, shadow=True)
plt.title(f'Polaridade: {polaridade}\n({categoria})', fontweight='bold', pad=20)

# 2. Gr√°fico de Subjetividade (Barras horizontais)
ax2 = plt.subplot(2, 3, 2)
subjetividade = dados_sentimento['subjetividade']
objetividade = 1 - subjetividade

categorias = ['Objetividade', 'Subjetividade']
valores = [objetividade, subjetividade]
cores_sub = ['#96ceb4', '#ffeaa7']

bars = plt.barh(categorias, valores, color=cores_sub, alpha=0.8, edgecolor='black', linewidth=1.5)
plt.xlim(0, 1)
plt.xlabel('Propor√ß√£o')
plt.title(f'Subjetividade: {subjetividade}', fontweight='bold', pad=20)

# Adicionar valores nas barras
for i, bar in enumerate(bars):
    width = bar.get_width()
    plt.text(width/2, bar.get_y() + bar.get_height()/2,
             f'{width:.1f}', ha='center', va='center', fontweight='bold', fontsize=12)

# 3. Gauge Chart para Polaridade
ax3 = plt.subplot(2, 3, 3)
theta = np.linspace(0, np.pi, 100)
r = 1

# Criar semic√≠rculo
x = r * np.cos(theta)
y = r * np.sin(theta)
plt.plot(x, y, 'k-', linewidth=3)

# Colorir se√ß√µes
theta_neg = np.linspace(0, np.pi/3, 50)
theta_neu = np.linspace(np.pi/3, 2*np.pi/3, 50)
theta_pos = np.linspace(2*np.pi/3, np.pi, 50)

plt.fill_between(np.cos(theta_neg), np.sin(theta_neg), alpha=0.3, color='#ff6b6b', label='Negativo')
plt.fill_between(np.cos(theta_neu), np.sin(theta_neu), alpha=0.3, color='#4ecdc4', label='Neutro')
plt.fill_between(np.cos(theta_pos), np.sin(theta_pos), alpha=0.3, color='#45b7d1', label='Positivo')

# Ponteiro para polaridade
angulo_ponteiro = np.pi/2 + (polaridade * np.pi/2)
x_ponteiro = 0.8 * np.cos(angulo_ponteiro)
y_ponteiro = 0.8 * np.sin(angulo_ponteiro)
plt.arrow(0, 0, x_ponteiro, y_ponteiro, head_width=0.1, head_length=0.1,
          fc='red', ec='red', linewidth=3)

plt.xlim(-1.2, 1.2)
plt.ylim(-0.2, 1.2)
plt.axis('equal')
plt.axis('off')
plt.title('Medidor de Polaridade', fontweight='bold', pad=20)
plt.legend(loc='upper right')

# 4. Gr√°fico de barras com os valores principais
ax4 = plt.subplot(2, 3, 4)
metricas = ['Polaridade', 'Subjetividade']
valores_metricas = [dados_sentimento['polaridade'], dados_sentimento['subjetividade']]
cores_metricas = ['#667eea', '#764ba2']

bars = plt.bar(metricas, valores_metricas, color=cores_metricas, alpha=0.8,
               edgecolor='black', linewidth=1.5)
plt.ylim(-1, 1)
plt.ylabel('Valor')
plt.title('M√©tricas Principais', fontweight='bold', pad=20)
plt.grid(True, alpha=0.3)

# Adicionar valores nas barras
for i, bar in enumerate(bars):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.02 if height >= 0 else height - 0.05,
             f'{height:.1f}', ha='center', va='bottom' if height >= 0 else 'top',
             fontweight='bold', fontsize=12)

# 5. Heatmap de resumo
ax5 = plt.subplot(2, 3, 5)
dados_heatmap = np.array([[dados_sentimento['polaridade']],
                         [dados_sentimento['subjetividade']]])
labels_heatmap = ['Polaridade', 'Subjetividade']

im = plt.imshow(dados_heatmap, cmap='RdYlBu', aspect='auto', vmin=-1, vmax=1)
plt.yticks(range(len(labels_heatmap)), labels_heatmap)
plt.xticks([])
plt.title('Heatmap dos Valores', fontweight='bold', pad=20)

# Adicionar valores no heatmap
for i in range(len(labels_heatmap)):
    plt.text(0, i, f'{dados_heatmap[i][0]:.1f}', ha='center', va='center',
             fontweight='bold', fontsize=14, color='white')

plt.colorbar(im, shrink=0.8)

# 6. Card de resultado final
ax6 = plt.subplot(2, 3, 6)
plt.axis('off')

# Criar um ret√¢ngulo colorido como fundo
if categoria == 'Positivo':
    cor_fundo = '#45b7d1'
elif categoria == 'Negativo':
    cor_fundo = '#ff6b6b'
else:
    cor_fundo = '#4ecdc4'

rect = plt.Rectangle((0.1, 0.3), 0.8, 0.4, facecolor=cor_fundo, alpha=0.8,
                    edgecolor='black', linewidth=2)
ax6.add_patch(rect)

# Texto do resultado
plt.text(0.5, 0.6, dados_sentimento['emoji'], ha='center', va='center',
         fontsize=40, transform=ax6.transAxes)
plt.text(0.5, 0.4, f"Sentimento:\n{dados_sentimento['sentimento']}",
         ha='center', va='center', fontsize=16, fontweight='bold',
         transform=ax6.transAxes)
plt.text(0.5, 0.1, f"P: {dados_sentimento['polaridade']:.1f} | S: {dados_sentimento['subjetividade']:.1f}",
         ha='center', va='center', fontsize=12, fontweight='bold',
         transform=ax6.transAxes)

plt.xlim(0, 1)
plt.ylim(0, 1)

# Ajustar layout e mostrar
plt.tight_layout()
plt.show()

# Imprimir resumo textual
print("="*50)
print("üìä RESUMO DA AN√ÅLISE DE SENTIMENTO")
print("="*50)
print(f"üéØ Polaridade: {dados_sentimento['polaridade']:.1f} ({categoria})")
print(f"üìù Subjetividade: {dados_sentimento['subjetividade']:.1f}")
print(f"üòä Sentimento: {dados_sentimento['sentimento']} {dados_sentimento['emoji']}")
print("-"*50)
print("üí° Interpreta√ß√£o:")
if dados_sentimento['polaridade'] == 0:
    print("   ‚Ä¢ Texto completamente neutro")
elif abs(dados_sentimento['polaridade']) < 0.1:
    print("   ‚Ä¢ Texto praticamente neutro")

if dados_sentimento['subjetividade'] < 0.2:
    print("   ‚Ä¢ Conte√∫do muito objetivo e factual")
elif dados_sentimento['subjetividade'] < 0.5:
    print("   ‚Ä¢ Conte√∫do mais objetivo que subjetivo")
else:
    print("   ‚Ä¢ Conte√∫do subjetivo com opini√µes")
print("="*50)

"""### Entrevista 2"""

# Passo 1: Instalar bibliotecas necess√°rias
!pip install textblob

# Passo 2: Importar bibliotecas
from textblob import TextBlob
from google.colab import files

# Passo 3: Fazer upload do arquivo .txt
uploaded = files.upload()

# Passo 4: Ler o conte√∫do do arquivo
for filename in uploaded.keys():
    with open(filename, 'r', encoding='utf-8') as file:
        texto = file.read()

# Passo 5: An√°lise de sentimento
blob = TextBlob(texto)

# Polarity: -1 (negativo) a 1 (positivo)
# Subjectivity: 0 (objetivo) a 1 (subjetivo)
print("Texto analisado:")
print(texto[:300] + "..." if len(texto) > 300 else texto)
print("\nAn√°lise de Sentimento:")
print(f"Polaridade: {blob.sentiment.polarity}")
print(f"Subjetividade: {blob.sentiment.subjectivity}")

# Classifica√ß√£o simples
if blob.sentiment.polarity > 0:
    print("Sentimento: Positivo üòä")
elif blob.sentiment.polarity < 0:
    print("Sentimento: Negativo üò†")
else:
    print("Sentimento: Neutro üòê")

"""#### Gr√°fico"""

# C√≥digo para Google Colab - Visualiza√ß√£o de An√°lise de Sentimento

# Importar bibliotecas necess√°rias
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from matplotlib.patches import Wedge
import warnings
warnings.filterwarnings('ignore')

# Configurar estilo dos gr√°ficos
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Dados da an√°lise de sentimento
dados_sentimento = {
    'polaridade': 0.07048611111111111,
    'subjetividade': 0.6277777777777778,
    'sentimento': 'Positivo',
    'emoji': 'üòä'
}

# Criar figura com subplots
fig = plt.figure(figsize=(16, 12))
fig.suptitle('üìä An√°lise de Sentimento - Dashboard', fontsize=20, fontweight='bold', y=0.95)

# 1. Gr√°fico de Polaridade (Pizza)
ax1 = plt.subplot(2, 3, 1)
polaridade = dados_sentimento['polaridade']

# Determinar categoria da polaridade
if polaridade < -0.1:
    valores_polaridade = [abs(polaridade), 0, 0]
    categoria = 'Negativo'
elif polaridade > 0.1:
    valores_polaridade = [0, 0, polaridade]
    categoria = 'Positivo'
else:
    valores_polaridade = [0, 1, 0]
    categoria = 'Neutro'

labels_polaridade = ['Negativo', 'Neutro', 'Positivo']
cores_polaridade = ['#ff6b6b', '#4ecdc4', '#45b7d1']
explode = (0.05, 0.05, 0.05)

plt.pie(valores_polaridade, labels=labels_polaridade, colors=cores_polaridade,
        explode=explode, autopct='%1.1f%%', startangle=90, shadow=True)
plt.title(f'Polaridade: {polaridade}\n({categoria})', fontweight='bold', pad=20)

# 2. Gr√°fico de Subjetividade (Barras horizontais)
ax2 = plt.subplot(2, 3, 2)
subjetividade = dados_sentimento['subjetividade']
objetividade = 1 - subjetividade

categorias = ['Objetividade', 'Subjetividade']
valores = [objetividade, subjetividade]
cores_sub = ['#96ceb4', '#ffeaa7']

bars = plt.barh(categorias, valores, color=cores_sub, alpha=0.8, edgecolor='black', linewidth=1.5)
plt.xlim(0, 1)
plt.xlabel('Propor√ß√£o')
plt.title(f'Subjetividade: {subjetividade}', fontweight='bold', pad=20)

# Adicionar valores nas barras
for i, bar in enumerate(bars):
    width = bar.get_width()
    plt.text(width/2, bar.get_y() + bar.get_height()/2,
             f'{width:.1f}', ha='center', va='center', fontweight='bold', fontsize=12)

# 3. Gauge Chart para Polaridade
ax3 = plt.subplot(2, 3, 3)
theta = np.linspace(0, np.pi, 100)
r = 1

# Criar semic√≠rculo
x = r * np.cos(theta)
y = r * np.sin(theta)
plt.plot(x, y, 'k-', linewidth=3)

# Colorir se√ß√µes
theta_neg = np.linspace(0, np.pi/3, 50)
theta_neu = np.linspace(np.pi/3, 2*np.pi/3, 50)
theta_pos = np.linspace(2*np.pi/3, np.pi, 50)

plt.fill_between(np.cos(theta_neg), np.sin(theta_neg), alpha=0.3, color='#ff6b6b', label='Negativo')
plt.fill_between(np.cos(theta_neu), np.sin(theta_neu), alpha=0.3, color='#4ecdc4', label='Neutro')
plt.fill_between(np.cos(theta_pos), np.sin(theta_pos), alpha=0.3, color='#45b7d1', label='Positivo')

# Ponteiro para polaridade
angulo_ponteiro = np.pi/2 + (polaridade * np.pi/2)
x_ponteiro = 0.8 * np.cos(angulo_ponteiro)
y_ponteiro = 0.8 * np.sin(angulo_ponteiro)
plt.arrow(0, 0, x_ponteiro, y_ponteiro, head_width=0.1, head_length=0.1,
          fc='red', ec='red', linewidth=3)

plt.xlim(-1.2, 1.2)
plt.ylim(-0.2, 1.2)
plt.axis('equal')
plt.axis('off')
plt.title('Medidor de Polaridade', fontweight='bold', pad=20)
plt.legend(loc='upper right')

# 4. Gr√°fico de barras com os valores principais
ax4 = plt.subplot(2, 3, 4)
metricas = ['Polaridade', 'Subjetividade']
valores_metricas = [dados_sentimento['polaridade'], dados_sentimento['subjetividade']]
cores_metricas = ['#667eea', '#764ba2']

bars = plt.bar(metricas, valores_metricas, color=cores_metricas, alpha=0.8,
               edgecolor='black', linewidth=1.5)
plt.ylim(-1, 1)
plt.ylabel('Valor')
plt.title('M√©tricas Principais', fontweight='bold', pad=20)
plt.grid(True, alpha=0.3)

# Adicionar valores nas barras
for i, bar in enumerate(bars):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.02 if height >= 0 else height - 0.05,
             f'{height:.1f}', ha='center', va='bottom' if height >= 0 else 'top',
             fontweight='bold', fontsize=12)

# 5. Heatmap de resumo
ax5 = plt.subplot(2, 3, 5)
dados_heatmap = np.array([[dados_sentimento['polaridade']],
                         [dados_sentimento['subjetividade']]])
labels_heatmap = ['Polaridade', 'Subjetividade']

im = plt.imshow(dados_heatmap, cmap='RdYlBu', aspect='auto', vmin=-1, vmax=1)
plt.yticks(range(len(labels_heatmap)), labels_heatmap)
plt.xticks([])
plt.title('Heatmap dos Valores', fontweight='bold', pad=20)

# Adicionar valores no heatmap
for i in range(len(labels_heatmap)):
    plt.text(0, i, f'{dados_heatmap[i][0]:.1f}', ha='center', va='center',
             fontweight='bold', fontsize=14, color='white')

plt.colorbar(im, shrink=0.8)

# 6. Card de resultado final
ax6 = plt.subplot(2, 3, 6)
plt.axis('off')

# Criar um ret√¢ngulo colorido como fundo
if categoria == 'Positivo':
    cor_fundo = '#45b7d1'
elif categoria == 'Negativo':
    cor_fundo = '#ff6b6b'
else:
    cor_fundo = '#4ecdc4'

rect = plt.Rectangle((0.1, 0.3), 0.8, 0.4, facecolor=cor_fundo, alpha=0.8,
                    edgecolor='black', linewidth=2)
ax6.add_patch(rect)

# Texto do resultado
plt.text(0.5, 0.6, dados_sentimento['emoji'], ha='center', va='center',
         fontsize=40, transform=ax6.transAxes)
plt.text(0.5, 0.4, f"Sentimento:\n{dados_sentimento['sentimento']}",
         ha='center', va='center', fontsize=16, fontweight='bold',
         transform=ax6.transAxes)
plt.text(0.5, 0.1, f"P: {dados_sentimento['polaridade']:.1f} | S: {dados_sentimento['subjetividade']:.1f}",
         ha='center', va='center', fontsize=12, fontweight='bold',
         transform=ax6.transAxes)

plt.xlim(0, 1)
plt.ylim(0, 1)

# Ajustar layout e mostrar
plt.tight_layout()
plt.show()

# Imprimir resumo textual
print("="*50)
print("üìä RESUMO DA AN√ÅLISE DE SENTIMENTO")
print("="*50)
print(f"üéØ Polaridade: {dados_sentimento['polaridade']:.3f} ({categoria})")
print(f"üìù Subjetividade: {dados_sentimento['subjetividade']:.3f}")
print(f"üòä Sentimento: {dados_sentimento['sentimento']} {dados_sentimento['emoji']}")
print("-"*50)
print("üí° Interpreta√ß√£o:")
if dados_sentimento['polaridade'] > 0.05:
    print("   ‚Ä¢ Texto com tend√™ncia positiva")
elif dados_sentimento['polaridade'] < -0.05:
    print("   ‚Ä¢ Texto com tend√™ncia negativa")
else:
    print("   ‚Ä¢ Texto neutro")

if dados_sentimento['subjetividade'] > 0.6:
    print("   ‚Ä¢ Conte√∫do altamente subjetivo com muitas opini√µes")
elif dados_sentimento['subjetividade'] > 0.4:
    print("   ‚Ä¢ Conte√∫do moderadamente subjetivo")
elif dados_sentimento['subjetividade'] > 0.2:
    print("   ‚Ä¢ Conte√∫do mais objetivo que subjetivo")
else:
    print("   ‚Ä¢ Conte√∫do muito objetivo e factual")
print("="*50)

"""#### Compara√ß√£o em Gr√°fico"""

# C√≥digo para Google Colab - Compara√ß√£o entre Entrevista 1 e Entrevista 2

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8')

# Dados das duas entrevistas
dados_entrevistas = {
    'Entrevista 1': {
        'polaridade': 0.0,
        'subjetividade': 0.1,
        'sentimento': 'Neutro',
        'emoji': 'üòê'
    },
    'Entrevista 2': {
        'polaridade': 0.07048611111111111,
        'subjetividade': 0.6277777777777778,
        'sentimento': 'Positivo',
        'emoji': 'üòä'
    }
}

# Criar figura com layout otimizado
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('üìä Compara√ß√£o de An√°lise de Sentimento: Entrevista 1 vs Entrevista 2',
             fontsize=16, fontweight='bold', y=0.95)

# 1. Gr√°fico de barras agrupadas - M√©tricas principais
entrevistas = ['Entrevista 1', 'Entrevista 2']
polaridades = [dados_entrevistas['Entrevista 1']['polaridade'],
               dados_entrevistas['Entrevista 2']['polaridade']]
subjetividades = [dados_entrevistas['Entrevista 1']['subjetividade'],
                  dados_entrevistas['Entrevista 2']['subjetividade']]

x = np.arange(len(entrevistas))
width = 0.35

bars1 = ax1.bar(x - width/2, polaridades, width, label='Polaridade',
                color='#3498db', alpha=0.8, edgecolor='black')
bars2 = ax1.bar(x + width/2, subjetividades, width, label='Subjetividade',
                color='#e74c3c', alpha=0.8, edgecolor='black')

ax1.set_xlabel('Entrevistas')
ax1.set_ylabel('Valores')
ax1.set_title('Polaridade vs Subjetividade', fontweight='bold')
ax1.set_xticks(x)
ax1.set_xticklabels(entrevistas)
ax1.legend()
ax1.grid(True, alpha=0.3, axis='y')
ax1.set_ylim(0, 0.7)

# Adicionar valores nas barras
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom',
                fontweight='bold', fontsize=10)

# 2. Gr√°fico de dispers√£o
ax2.scatter(dados_entrevistas['Entrevista 1']['polaridade'],
           dados_entrevistas['Entrevista 1']['subjetividade'],
           s=400, c='#3498db', alpha=0.7, edgecolors='black', linewidth=2,
           label='Entrevista 1', marker='o')
ax2.scatter(dados_entrevistas['Entrevista 2']['polaridade'],
           dados_entrevistas['Entrevista 2']['subjetividade'],
           s=400, c='#e74c3c', alpha=0.7, edgecolors='black', linewidth=2,
           label='Entrevista 2', marker='s')

# Adicionar emojis como anota√ß√µes
ax2.annotate('üòê', xy=(dados_entrevistas['Entrevista 1']['polaridade'],
                      dados_entrevistas['Entrevista 1']['subjetividade']),
             xytext=(0, 0), textcoords='offset points', fontsize=25, ha='center')
ax2.annotate('üòä', xy=(dados_entrevistas['Entrevista 2']['polaridade'],
                      dados_entrevistas['Entrevista 2']['subjetividade']),
             xytext=(0, 0), textcoords='offset points', fontsize=25, ha='center')

ax2.set_xlabel('Polaridade')
ax2.set_ylabel('Subjetividade')
ax2.set_title('Mapa Polaridade vs Subjetividade', fontweight='bold')
ax2.grid(True, alpha=0.3)
ax2.legend()
ax2.set_xlim(-0.05, 0.12)
ax2.set_ylim(0, 0.7)

# Adicionar linhas de refer√™ncia
ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Subjetividade M√©dia')
ax2.axvline(x=0, color='gray', linestyle='--', alpha=0.5, label='Neutralidade')

# 3. Gr√°fico de radar comparativo
categories = ['Polaridade\n(Normalizada)', 'Subjetividade', 'Positividade']
N = len(categories)

# Preparar valores para o radar
valores_ent1 = [
    (dados_entrevistas['Entrevista 1']['polaridade'] + 1) / 2,  # Normalizar para 0-1
    dados_entrevistas['Entrevista 1']['subjetividade'],
    max(0, dados_entrevistas['Entrevista 1']['polaridade'])
]
valores_ent2 = [
    (dados_entrevistas['Entrevista 2']['polaridade'] + 1) / 2,
    dados_entrevistas['Entrevista 2']['subjetividade'],
    max(0, dados_entrevistas['Entrevista 2']['polaridade'])
]

# √Çngulos para cada eixo
angles = [n / float(N) * 2 * np.pi for n in range(N)]
angles += angles[:1]

# Fechar o pol√≠gono
valores_ent1 += valores_ent1[:1]
valores_ent2 += valores_ent2[:1]

ax3 = plt.subplot(2, 2, 3, projection='polar')
ax3.plot(angles, valores_ent1, 'o-', linewidth=2, label='Entrevista 1', color='#3498db')
ax3.fill(angles, valores_ent1, alpha=0.25, color='#3498db')
ax3.plot(angles, valores_ent2, 's-', linewidth=2, label='Entrevista 2', color='#e74c3c')
ax3.fill(angles, valores_ent2, alpha=0.25, color='#e74c3c')

ax3.set_xticks(angles[:-1])
ax3.set_xticklabels(categories)
ax3.set_ylim(0, 1)
ax3.set_title('Perfil Comparativo das Entrevistas', fontweight='bold', pad=20)
ax3.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))

# 4. Cards resumo das entrevistas
ax4.axis('off')

# Card Entrevista 1
rect1 = plt.Rectangle((0.05, 0.55), 0.4, 0.4, facecolor='#3498db', alpha=0.8,
                     edgecolor='black', linewidth=2, transform=ax4.transAxes)
ax4.add_patch(rect1)

ax4.text(0.25, 0.85, 'üòê', ha='center', va='center', fontsize=30, transform=ax4.transAxes)
ax4.text(0.25, 0.75, 'Entrevista 1', ha='center', va='center', fontsize=12,
         fontweight='bold', transform=ax4.transAxes, color='white')
ax4.text(0.25, 0.68, 'Neutro', ha='center', va='center', fontsize=11,
         fontweight='bold', transform=ax4.transAxes, color='white')
ax4.text(0.25, 0.61, f'P: {dados_entrevistas["Entrevista 1"]["polaridade"]:.3f}',
         ha='center', va='center', fontsize=9, fontweight='bold',
         transform=ax4.transAxes, color='white')
ax4.text(0.25, 0.57, f'S: {dados_entrevistas["Entrevista 1"]["subjetividade"]:.3f}',
         ha='center', va='center', fontsize=9, fontweight='bold',
         transform=ax4.transAxes, color='white')

# Card Entrevista 2
rect2 = plt.Rectangle((0.55, 0.55), 0.4, 0.4, facecolor='#e74c3c', alpha=0.8,
                     edgecolor='black', linewidth=2, transform=ax4.transAxes)
ax4.add_patch(rect2)

ax4.text(0.75, 0.85, 'üòä', ha='center', va='center', fontsize=30, transform=ax4.transAxes)
ax4.text(0.75, 0.75, 'Entrevista 2', ha='center', va='center', fontsize=12,
         fontweight='bold', transform=ax4.transAxes, color='white')
ax4.text(0.75, 0.68, 'Positivo', ha='center', va='center', fontsize=11,
         fontweight='bold', transform=ax4.transAxes, color='white')
ax4.text(0.75, 0.61, f'P: {dados_entrevistas["Entrevista 2"]["polaridade"]:.3f}',
         ha='center', va='center', fontsize=9, fontweight='bold',
         transform=ax4.transAxes, color='white')
ax4.text(0.75, 0.57, f'S: {dados_entrevistas["Entrevista 2"]["subjetividade"]:.3f}',
         ha='center', va='center', fontsize=9, fontweight='bold',
         transform=ax4.transAxes, color='white')

# Compara√ß√£o textual
ax4.text(0.5, 0.45, 'COMPARA√á√ÉO', ha='center', va='center', fontsize=14,
         fontweight='bold', transform=ax4.transAxes)
ax4.text(0.5, 0.35, 'Entrevista 1: Texto objetivo e neutro', ha='center', va='center',
         fontsize=10, transform=ax4.transAxes)
ax4.text(0.5, 0.30, 'Entrevista 2: Texto subjetivo e positivo', ha='center', va='center',
         fontsize=10, transform=ax4.transAxes)

# Diferen√ßas
diff_pol = dados_entrevistas['Entrevista 2']['polaridade'] - dados_entrevistas['Entrevista 1']['polaridade']
diff_sub = dados_entrevistas['Entrevista 2']['subjetividade'] - dados_entrevistas['Entrevista 1']['subjetividade']

ax4.text(0.5, 0.20, f'Œî Polaridade: +{diff_pol:.3f}', ha='center', va='center',
         fontsize=10, fontweight='bold', transform=ax4.transAxes, color='green')
ax4.text(0.5, 0.15, f'Œî Subjetividade: +{diff_sub:.3f}', ha='center', va='center',
         fontsize=10, fontweight='bold', transform=ax4.transAxes, color='orange')

ax4.text(0.5, 0.05, 'üòê ‚Üí üòä', ha='center', va='center', fontsize=20, transform=ax4.transAxes)

plt.tight_layout()
plt.show()

# Criar DataFrame para exibir os dados
df_entrevistas = pd.DataFrame({
    'Entrevista': ['Entrevista 1', 'Entrevista 2'],
    'Polaridade': [dados_entrevistas['Entrevista 1']['polaridade'],
                   dados_entrevistas['Entrevista 2']['polaridade']],
    'Subjetividade': [dados_entrevistas['Entrevista 1']['subjetividade'],
                      dados_entrevistas['Entrevista 2']['subjetividade']],
    'Sentimento': [dados_entrevistas['Entrevista 1']['sentimento'],
                   dados_entrevistas['Entrevista 2']['sentimento']],
    'Emoji': [dados_entrevistas['Entrevista 1']['emoji'],
              dados_entrevistas['Entrevista 2']['emoji']]
})

print("="*60)
print("üìä COMPARA√á√ÉO ENTRE ENTREVISTAS")
print("="*60)
print(df_entrevistas.to_string(index=False))
print("="*60)
print(f"üéØ INTERPRETA√á√ÉO:")
print(f"   ‚Ä¢ Entrevista 1: Discurso neutro e objetivo (factual)")
print(f"   ‚Ä¢ Entrevista 2: Discurso positivo e subjetivo (opinativo)")
print(f"   ‚Ä¢ A segunda entrevista mostra mais emo√ß√£o e opini√£o pessoal")
print(f"   ‚Ä¢ Diferen√ßa na polaridade: +{diff_pol:.3f} (mais positiva)")
print(f"   ‚Ä¢ Diferen√ßa na subjetividade: +{diff_sub:.3f} (muito mais subjetiva)")
print("="*60)